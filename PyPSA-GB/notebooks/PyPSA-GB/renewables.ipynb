{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgb\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import renewables_ninja_data_analysis\n",
    "import snapshots\n",
    "import distance_calculator as dc\n",
    "import data_reader_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_REPD():\n",
    "    \"\"\"reads the REDP (renewable energy planning database) and converts to dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    currently none, as file is fixed below\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        dataframe of REPD with important fields\n",
    "    \"\"\"\n",
    "\n",
    "    # name of file here\n",
    "    file = '../data/renewables/renewable-energy-planning-database-q1-march-2021.csv'\n",
    "\n",
    "    # only important fields\n",
    "    fields = ['Site Name', 'Technology Type', 'Installed Capacity (MWelec)',\n",
    "              'CHP Enabled', 'Country',\n",
    "              'Turbine Capacity (MW)', 'No. of Turbines', 'Height of Turbines (m)',\n",
    "              'Mounting Type for Solar', 'Development Status',\n",
    "              'X-coordinate', 'Y-coordinate', 'Operational']\n",
    "    # reads csv\n",
    "    df = pd.read_csv(file, encoding='unicode_escape', usecols=fields,\n",
    "                     lineterminator='\\n')\n",
    "    # remove northern island sites\n",
    "    df.drop(df[df['Country'] == 'Northern Ireland'].index, inplace=True)\n",
    "    # then drop the country column\n",
    "    df.drop(columns=['Country'], inplace=True)\n",
    "    # only want operational sites\n",
    "    df = df.loc[df['Development Status'] == 'Operational']\n",
    "    # drop the entries without a capacity value or a given technology type or site name\n",
    "    df = df.dropna(subset=['Installed Capacity (MWelec)', 'Technology Type', 'Site Name'])\n",
    "    # reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # check for missing location data\n",
    "    if df['X-coordinate'].isnull().values.any() is True:\n",
    "        raise Exception(\"All X-coordinate values must be provided, please fill in missing data in csv file\")\n",
    "    if df['Y-coordinate'].isnull().values.any() is True:\n",
    "        raise Exception(\"All Y-coordinate values must be provided, please fill in missing data in csv file\")\n",
    "\n",
    "    # create two lists of conversions from OSGB to lat/lon\n",
    "    lon = []\n",
    "    lat = []\n",
    "    for i in range(len(df.index)):\n",
    "        x = df['X-coordinate'][i]\n",
    "        y = df['Y-coordinate'][i]\n",
    "        coord = osgb.grid_to_ll(x, y)\n",
    "        lat.append(coord[0])\n",
    "        lon.append(coord[1])\n",
    "    df['lon'] = lon\n",
    "    df['lat'] = lat\n",
    "\n",
    "    # different technology types\n",
    "    # currently no capacity of the following:\n",
    "    # Advanced Conversion Technology, Fuel Cell (Hydrogen), Hot Dry Rocks (HDR),\n",
    "\n",
    "    # print(df.loc[df['Technology Type'] == 'Advanced Conversion Technology'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Anaerobic Digestion'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Biomass (co-firing)'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Biomass (dedicated)'])\n",
    "    # print(df.loc[df['Technology Type'] == 'EfW Incineration'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Fuel Cell (Hydrogen)'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Hot Dry Rocks (HDR)'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Landfill Gas'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Large Hydro'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Pumped Storage Hydroelectricity'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Sewage Sludge Digestion'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Shoreline Wave'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Small Hydro'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Solar Photovoltaics'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Tidal Barrage and Tidal Stream'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Wind Offshore'])\n",
    "    # print(df.loc[df['Technology Type'] == 'Wind Onshore'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REPD_date_corrected(year):\n",
    "    \"\"\"corrects the REDP (renewable energy planning database) according to year of simulation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        dataframe of REPD filtered by operational in year of simulation\n",
    "    \"\"\"\n",
    "\n",
    "    df = read_REPD()\n",
    "    df2 = df['Operational']\n",
    "    df2 = pd.to_datetime(df2, format='%d/%m/%Y').dt.to_period('D')\n",
    "    # cut off is the end of the year being simulated\n",
    "    date = '31/12/' + str(year)\n",
    "    df = df[~(df2 > date)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda85c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_timeseries_res_for_year(path, year, tech, future):\n",
    "    \"\"\"fixes the timeseries of renewable profiles according to year\n",
    "\n",
    "    looks at the REPD and filters out from the timeseries those which\n",
    "    were not operational on they year to be modelled.\n",
    "    Note that in another function values are set to zero according to\n",
    "    the date operational within this year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The file path location of the timeseries\n",
    "    year: int/str\n",
    "        Year of simulation\n",
    "    tech: str\n",
    "        Technology type, e.g. wind offshore, solar photovoltaics\n",
    "    future: bool\n",
    "        Is the year to be modelled in future or not, e.g., future is true, past is false\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        the fixed timeseries\n",
    "    \"\"\"\n",
    "    if tech == 'Solar Photovoltaics':\n",
    "        # the solar outputs csv files needed to be split up so this appends them together again\n",
    "        path = '../data/renewables/atlite/outputs/PV/PV_' + str(year) + '_1' + '.csv'\n",
    "        df1 = pd.read_csv(path, index_col=0)\n",
    "        for c in range(2, 5):\n",
    "            path = '../data/renewables/atlite/outputs/PV/PV_' + str(year) + '_' + str(c) + '.csv'\n",
    "            df = pd.read_csv(path, index_col=0, header=None)\n",
    "            df.columns = df1.columns\n",
    "            df1 = pd.concat([df1, df])\n",
    "\n",
    "    else:\n",
    "        # just read csv file using given path\n",
    "        df1 = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    df1.index = pd.to_datetime(df1.index)\n",
    "    # want to ensure no duplicate names\n",
    "    cols = pd.Series(df1.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    # rename the columns with the cols list.\n",
    "    df1.columns = cols\n",
    "    df1.columns = df1.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df1.columns = df1.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "    df1.columns = df1.columns.astype(str).str.replace('ì', 'i')\n",
    "    df1.columns = df1.columns.str.strip()\n",
    "\n",
    "    # only filter historical years, for future years want all the RES units\n",
    "    if future is False:\n",
    "\n",
    "        df_res = REPD_date_corrected(year)\n",
    "        df_tech = df_res.loc[df_res['Technology Type'] == tech]\n",
    "        df_tech['Site Name'] = df_tech['Site Name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "        df_tech['Site Name'] = df_tech['Site Name'].astype(str).str.replace(u'\\xa0', '')\n",
    "        df_tech['Site Name'] = df_tech['Site Name'].astype(str).str.replace('ì', 'i')\n",
    "        df_tech['Site Name'] = df_tech['Site Name'].str.strip()\n",
    "\n",
    "        # check for duplicates\n",
    "        # check names are unique\n",
    "        duplicateDFRow = df_tech[df_tech.duplicated(['Site Name'], keep='first')]\n",
    "        # print(duplicateDFRow)\n",
    "        # rename duplicates\n",
    "        for i in range(len(duplicateDFRow.index.values)):\n",
    "            # print(df_tech['name'][duplicateDFRow.index.values[i]])\n",
    "            df_tech.at[duplicateDFRow.index.values[i], 'Site Name'] = (\n",
    "                df_tech['Site Name'][duplicateDFRow.index.values[i]] + '.1')\n",
    "            # print(df['name'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "        # narrow dataframe timeseries to those operational in the required year\n",
    "        # print(df1)\n",
    "        # print(len(df_tech['Operational'].values))\n",
    "        df1 = df1[df_tech['Site Name'].values]\n",
    "        # print(df1)\n",
    "        # print('here?')\n",
    "\n",
    "        # also want to return zeroes for before date\n",
    "        # change to datetime to compare\n",
    "        df2 = pd.to_datetime(df_tech['Operational'], dayfirst=True).dt.to_period('d')\n",
    "        # df_tech['date'] = df2\n",
    "        # df_tech.loc['date'] = df2\n",
    "        mask = df2 > '01/01/' + str(year)\n",
    "        # filtered df to just the year in question\n",
    "        filtered_df = df_tech.loc[mask]\n",
    "        # list of sites to change timeseries\n",
    "        sites = filtered_df['Site Name'].values\n",
    "        # change value of the timeseries based on date operational\n",
    "        length = len(sites)\n",
    "\n",
    "        for name in range(length):\n",
    "\n",
    "            # get the operational date and convert to datetime\n",
    "            date_operational = pd.to_datetime(filtered_df['Operational'].values[name], dayfirst=True)\n",
    "            # times before operational dates set to zero\n",
    "            df1[sites[name]].loc[df1.index[0]:date_operational] = 0.\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c587ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hydro(year):\n",
    "    \"\"\"reads hydro data from REDP\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        dataframe of hydro data\n",
    "    \"\"\"\n",
    "\n",
    "    df = REPD_date_corrected(year)\n",
    "    df1 = df.loc[df['Technology Type'] == 'Large Hydro']\n",
    "    df2 = df.loc[df['Technology Type'] == 'Small Hydro']\n",
    "    df3 = df.loc[df['Technology Type'] == 'Pumped Storage Hydroelectricity']\n",
    "    df_REDP = pd.concat([df1, df2, df3], ignore_index=True, sort=False)\n",
    "    df_REDP = df_REDP.rename(columns={'Site Name': 'name', 'Technology Type': 'type',\n",
    "                                      'Installed Capacity (MWelec)': 'p_nom'})\n",
    "    df_REDP = df_REDP[['name', 'type', 'p_nom', 'lat', 'lon']]\n",
    "    # print(df_REDP)\n",
    "\n",
    "    file = '../data/renewables/hydro_DUKES_2020.csv'\n",
    "    df_dukes = pd.read_csv(file, encoding='unicode_escape')\n",
    "    df_dukes.loc[:, 'Geocoordinates'] = df_dukes['Geocoordinates'].str.replace(',', '')\n",
    "    df_dukes.loc[:, 'Geocoordinates'] = df_dukes['Geocoordinates'].str.split()\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    for i in range(len(df_dukes['Geocoordinates'])):\n",
    "        lat.append(df_dukes['Geocoordinates'].values[i][0])\n",
    "        lon.append(df_dukes['Geocoordinates'].values[i][1])\n",
    "\n",
    "    df_dukes['lat'] = lat\n",
    "    df_dukes['lon'] = lon\n",
    "\n",
    "    df_dukes = df_dukes.rename(columns={'Station Name': 'name', 'Type': 'type',\n",
    "                                        'Installed Capacity (MW)': 'p_nom'})\n",
    "    df_dukes = df_dukes[['name', 'type', 'p_nom', 'lat', 'lon']]\n",
    "\n",
    "    df_hydro = pd.concat([df_dukes, df_REDP], ignore_index=True, sort=False)\n",
    "\n",
    "    df_hydro2 = df_hydro.drop_duplicates(subset=['name'])\n",
    "\n",
    "    conditions = [\n",
    "        (df_hydro2['p_nom'] > 5.0) & (df_hydro2['type'] != 'Pumped Storage Hydroelectricity'),\n",
    "        (df_hydro2['p_nom'] <= 5.0) & (df_hydro2['type'] != 'Pumped Storage Hydroelectricity'),\n",
    "        (df_hydro2['type'] == 'Pumped Storage Hydroelectricity')]\n",
    "    type_ = ['Large Hydro', 'Small Hydro', 'Pumped Storage Hydroelectricity']\n",
    "    df_hydro2.loc[:, 'type'] = np.select(conditions, type_)\n",
    "    df_hydro2 = df_hydro2.reset_index(drop=True)\n",
    "    df_hydro2['carrier'] = df_hydro2['type']\n",
    "\n",
    "    return df_hydro2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45003291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hydro_time_series(year):\n",
    "    \"\"\"reads hydro timeseries as saved in relevant folder\n",
    "\n",
    "    currently this is reading ELEXON data, see the read_csv below\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        contains two dataframes of unnorm and normalised timeseries for hydro output\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv('../data/renewables/generation_2015-02-22_2020-12-30_ELEXON.csv')\n",
    "    dti = pd.date_range(start='2015-02-22 00:00:00', end='2020-12-31 23:30:00', freq='0.5h')\n",
    "    df = df.set_index(dti)\n",
    "    df_hydro = df[['npshyd']]\n",
    "\n",
    "    # want to distribute this among all of the non-pumped hydro generators\n",
    "    df2 = read_hydro(year)\n",
    "    df2.index = df2['name']\n",
    "    # delete pumped storage\n",
    "    df2 = df2[~df2['type'].isin(['Pumped Storage Hydroelectricity'])]\n",
    "    total_capacity = df2['p_nom'].sum()\n",
    "    # add a normalised value for each hydro scheme\n",
    "    df2['normalised'] = df2['p_nom'] / total_capacity\n",
    "    # multiply the normalised by the time series to get\n",
    "    # time series for each hydro scheme\n",
    "    total_time_series = df_hydro['npshyd'].values\n",
    "\n",
    "    df_hydro_norm = pd.DataFrame(index=df_hydro.index)\n",
    "\n",
    "    for i in range(len(df2)):\n",
    "\n",
    "        name = df2.index[i]\n",
    "        df_hydro.loc[:, name] = (df2.loc[name, 'normalised'] * total_time_series).copy()\n",
    "        df_hydro_norm.loc[:, name] = (\n",
    "            df2.loc[name, 'normalised'] * total_time_series / df2.loc[name, 'p_nom']).copy()\n",
    "\n",
    "    # drop the total column\n",
    "    df_hydro = df_hydro.drop(columns=['npshyd'])\n",
    "\n",
    "    return {'time_series': df_hydro, 'time_series_norm': df_hydro_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_non_dispatchable_continuous(year):\n",
    "    \"\"\"reads the continuous renewable generators from REPD and converts to dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        data on continuous renewable generators\n",
    "    \"\"\"\n",
    "\n",
    "    df = REPD_date_corrected(year)\n",
    "    df1 = df.loc[df['Technology Type'] == 'Anaerobic Digestion']\n",
    "    df2 = df.loc[df['Technology Type'] == 'EfW Incineration']\n",
    "    df3 = df.loc[df['Technology Type'] == 'Landfill Gas']\n",
    "    df4 = df.loc[df['Technology Type'] == 'Sewage Sludge Digestion']\n",
    "    df5 = df.loc[df['Technology Type'] == 'Shoreline Wave']\n",
    "    df6 = df.loc[df['Technology Type'] == 'Tidal Barrage and Tidal Stream']\n",
    "    df_NDC = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True, sort=False)\n",
    "\n",
    "    df_NDC = df_NDC.rename(columns={'Site Name': 'name', 'Technology Type': 'type',\n",
    "                                    'Installed Capacity (MWelec)': 'p_nom'})\n",
    "    df_NDC = df_NDC[['name', 'type', 'p_nom', 'lat', 'lon']]\n",
    "    df_NDC['carrier'] = df_NDC['type']\n",
    "    return df_NDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_biomass(year):\n",
    "    \"\"\"reads the biomass generators from REPD and converts to dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        data on biomass\n",
    "    \"\"\"\n",
    "\n",
    "    df = REPD_date_corrected(year)\n",
    "    df1 = df.loc[df['Technology Type'] == 'Biomass (co-firing)']\n",
    "    df2 = df.loc[df['Technology Type'] == 'Biomass (dedicated)']\n",
    "    df_biomass = pd.concat([df1, df2], ignore_index=True, sort=False)\n",
    "\n",
    "    df_biomass = df_biomass.rename(columns={'Site Name': 'name', 'Technology Type': 'type',\n",
    "                                            'Installed Capacity (MWelec)': 'p_nom'})\n",
    "    df_biomass = df_biomass[['name', 'type', 'p_nom', 'lat', 'lon']]\n",
    "    df_biomass['carrier'] = df_biomass['type']\n",
    "    return df_biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_biomass_p_nom(year, scenario, FES):\n",
    "\n",
    "    tech = 'Biomass'\n",
    "    future_capacities_dict = future_RES_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    t1 = 'Biomass (co-firing)'\n",
    "    t2 = 'Biomass (dedicated)'\n",
    "\n",
    "    # get generators dataframe with p_noms to be scaled\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech1 = generators.loc[generators['carrier'] == t1]\n",
    "    gen_tech2 = generators.loc[generators['carrier'] == t2]\n",
    "    gen_tech = pd.concat([gen_tech1, gen_tech2])\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC1 = generators_UC.loc[generators_UC['carrier'] == t1]\n",
    "    gen_tech_UC2 = generators_UC.loc[generators_UC['carrier'] == t2]\n",
    "    gen_tech_UC = pd.concat([gen_tech_UC1, gen_tech_UC2])\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to remove the original tech\n",
    "    # but first keep Biomass CCS\n",
    "    biomass_CCS = generators.loc[generators['carrier'] == 'CCS Biomass']\n",
    "    biomass_CCS_UC = generators_UC.loc[generators_UC['carrier'] == 'CCS Biomass']\n",
    "\n",
    "    generators = generators[~generators.carrier.str.contains(tech)]\n",
    "    generators_UC = generators_UC[~generators_UC.carrier.str.contains(tech)]\n",
    "\n",
    "    # then add the new p_nom tech as well as CCS Biomass\n",
    "    generators = pd.concat([generators, gen_tech, biomass_CCS])\n",
    "\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC, biomass_CCS_UC])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tidal_lagoon(year, scenario, fes):\n",
    "\n",
    "    df_tidal_lagoon = pd.read_excel('../data/renewables/Marine/tidal_lagoon_future_deployment_scenarios.xlsx', sheet_name=None)\n",
    "    # print(df_tidal_lagoon)\n",
    "\n",
    "    if scenario == 'Leading The Way':\n",
    "        sheet_name = 'tidal_lagoon_LW_FES' + str(fes)\n",
    "    elif scenario == 'Consumer Transformation':\n",
    "        sheet_name = 'tidal_lagoon_CT_FES' + str(fes)\n",
    "    elif scenario == 'System Transformation':\n",
    "        sheet_name = 'tidal_lagoon_ST_FES' + str(fes)\n",
    "    elif scenario == 'Steady Progression':\n",
    "        sheet_name = 'tidal_lagoon_SP_FES' + str(fes)\n",
    "    elif scenario == 'Falling Short':\n",
    "        sheet_name = 'tidal_lagoon_FS_FES' + str(fes)\n",
    "\n",
    "    df_tidal_lagoon_capacities = df_tidal_lagoon[sheet_name].T\n",
    "    df_tidal_lagoon_capacities.columns = df_tidal_lagoon_capacities.iloc[0]\n",
    "    df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.drop(['Lat', 'Lon', 'Site ID', 'Site Name'])\n",
    "    df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.drop(columns=['Total'])\n",
    "\n",
    "    if fes == 2021:\n",
    "        # need to get values for years 2020 - 2050\n",
    "        index_ = ['2025-01-01', '2030-01-01', '2035-01-01', '2040-01-01', '2045-01-01', '2050-01-01']\n",
    "        df_tidal_lagoon_capacities.index = index_\n",
    "        df_tidal_lagoon_capacities.index = pd.to_datetime(df_tidal_lagoon_capacities.index, utc=True)\n",
    "        df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.resample('12MS').asfreq()\n",
    "        df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.astype(float)\n",
    "        df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.interpolate(method='linear', limit_direction='forward')  \n",
    "    df_tidal_lagoon_capacities = df_tidal_lagoon_capacities.fillna(0)\n",
    "\n",
    "    df_tidal_lagoon_locations = df_tidal_lagoon[sheet_name].iloc[:, -3:]\n",
    "    df_tidal_lagoon_locations.index = df_tidal_lagoon[sheet_name]['Site Name']\n",
    "    # drop the last row as this is a total row\n",
    "    df_tidal_lagoon_locations.drop(df_tidal_lagoon_locations.tail(1).index, inplace=True)\n",
    "    df_tidal_lagoon_locations.rename(columns={'Lat': 'lat', 'Lon': 'lon'}, inplace=True)\n",
    "\n",
    "    # need to use lat and lon to figure out the nearest bus - then add column called bus\n",
    "    df = df_tidal_lagoon_locations.rename(columns={'lat': 'y', 'lon': 'x'})\n",
    "    buses = dc.map_to_bus(df)\n",
    "    df_tidal_lagoon_locations['bus'] = buses\n",
    "\n",
    "    if year < 2025:\n",
    "        year = 2025\n",
    "    if fes == 2021:\n",
    "        date = str(year) + '-01-01'\n",
    "    else:\n",
    "        date = year\n",
    "    dic = {'capacities': df_tidal_lagoon_capacities.loc[date, :], 'locations': df_tidal_lagoon_locations}\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tidal_stream(year, scenario, fes):\n",
    "\n",
    "    df_tidal_stream = pd.read_excel('../data/renewables/Marine/tidal_stream_future_deployment_scenarios.xlsx', sheet_name=None)\n",
    "    # print(df_tidal_stream)\n",
    "\n",
    "    if scenario == 'Leading The Way':\n",
    "        sheet_name = 'tidal_stream_LW_FES' + str(fes)\n",
    "    elif scenario == 'Consumer Transformation':\n",
    "        sheet_name = 'tidal_stream_CT_FES' + str(fes)\n",
    "    elif scenario == 'System Transformation':\n",
    "        sheet_name = 'tidal_stream_ST_FES' + str(fes)\n",
    "    elif scenario == 'Steady Progression':\n",
    "        sheet_name = 'tidal_stream_SP_FES' + str(fes)\n",
    "    elif scenario == 'Falling Short':\n",
    "        sheet_name = 'tidal_stream_FS_FES' + str(fes)\n",
    "\n",
    "    df_tidal_stream_capacities = df_tidal_stream[sheet_name].T\n",
    "    df_tidal_stream_capacities.columns = df_tidal_stream_capacities.iloc[1]\n",
    "    df_tidal_stream_capacities = df_tidal_stream_capacities.drop(['Lat', 'Lon', 'Site ID', 'Site Name'])\n",
    "    df_tidal_stream_capacities = df_tidal_stream_capacities.iloc[:, :-1]\n",
    "\n",
    "    if fes == 2021:\n",
    "        # need to get values for years 2020 - 2050\n",
    "        index_ = ['2025-01-01', '2030-01-01', '2035-01-01', '2040-01-01', '2045-01-01', '2050-01-01']\n",
    "        df_tidal_stream_capacities.index = index_\n",
    "        df_tidal_stream_capacities.index = pd.to_datetime(df_tidal_stream_capacities.index, utc=True)\n",
    "        df_tidal_stream_capacities = df_tidal_stream_capacities.resample('12MS').asfreq()\n",
    "        df_tidal_stream_capacities = df_tidal_stream_capacities.astype(float)\n",
    "        df_tidal_stream_capacities = df_tidal_stream_capacities.interpolate(method='linear', limit_direction='forward')\n",
    "    # replace NaN with zero\n",
    "    df_tidal_stream_capacities = df_tidal_stream_capacities.fillna(0)\n",
    "\n",
    "    df_tidal_stream_locations = df_tidal_stream[sheet_name].iloc[:, -3:]\n",
    "    df_tidal_stream_locations.index = df_tidal_stream[sheet_name]['Site ID']\n",
    "    # drop the last row as this is a total row\n",
    "    df_tidal_stream_locations.drop(df_tidal_stream_locations.tail(1).index, inplace=True)\n",
    "    df_tidal_stream_locations.rename(columns={'Lat': 'lat', 'Lon': 'lon'}, inplace=True)\n",
    "\n",
    "    # need to use lat and lon to figure out the nearest bus - then add column called bus\n",
    "    df = df_tidal_stream_locations.rename(columns={'lat': 'y', 'lon': 'x'})\n",
    "    buses = dc.map_to_bus(df)\n",
    "    df_tidal_stream_locations['bus'] = buses\n",
    "\n",
    "    if year < 2025:\n",
    "        year = 2025\n",
    "    if fes == 2021:\n",
    "        date = str(year) + '-01-01'\n",
    "    else:\n",
    "        date = year\n",
    "    dic = {'capacities': df_tidal_stream_capacities.loc[date, :], 'locations': df_tidal_stream_locations}\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wave_power(year, scenario, fes):\n",
    "\n",
    "    df_wave_power = pd.read_excel('../data/renewables/Marine/wave_power_future_deployment_scenarios.xlsx', sheet_name=None)\n",
    "    # print(df_wave_power)\n",
    "\n",
    "    if scenario == 'Leading The Way':\n",
    "        sheet_name = 'wave_power_LW_FES' + str(fes)\n",
    "    elif scenario == 'Consumer Transformation':\n",
    "        sheet_name = 'wave_power_CT_FES' + str(fes)\n",
    "    elif scenario == 'System Transformation':\n",
    "        sheet_name = 'wave_power_ST_FES' + str(fes)\n",
    "    elif scenario == 'Steady Progression':\n",
    "        sheet_name = 'wave_power_SP_FES' + str(fes)\n",
    "    elif scenario == 'Falling Short':\n",
    "        sheet_name = 'wave_power_FS_FES' + str(fes)\n",
    "\n",
    "    df_wave_power_capacities = df_wave_power[sheet_name].T\n",
    "    df_wave_power_capacities.columns = df_wave_power_capacities.iloc[1]\n",
    "    df_wave_power_capacities = df_wave_power_capacities.drop(['Lat', 'Lon', 'Site ID', 'Site Name'])\n",
    "    df_wave_power_capacities = df_wave_power_capacities.iloc[:, :-1]\n",
    "\n",
    "    if fes == 2021:\n",
    "        # need to get values for years 2020 - 2050\n",
    "        index_ = ['2025-01-01', '2030-01-01', '2035-01-01', '2040-01-01', '2045-01-01', '2050-01-01']\n",
    "        df_wave_power_capacities.index = index_\n",
    "        df_wave_power_capacities.index = pd.to_datetime(df_wave_power_capacities.index, utc=True)\n",
    "        df_wave_power_capacities = df_wave_power_capacities.resample('12MS').asfreq()\n",
    "        df_wave_power_capacities = df_wave_power_capacities.astype(float)\n",
    "        df_wave_power_capacities = df_wave_power_capacities.interpolate(method='linear', limit_direction='forward')\n",
    "    # replace NaN with zero\n",
    "    df_wave_power_capacities = df_wave_power_capacities.fillna(0)\n",
    "\n",
    "    df_wave_power_locations = df_wave_power[sheet_name].iloc[:, -3:]\n",
    "    df_wave_power_locations.index = df_wave_power[sheet_name]['Site ID']\n",
    "    # drop the last row as this is a total row\n",
    "    df_wave_power_locations.drop(df_wave_power_locations.tail(1).index, inplace=True)\n",
    "    df_wave_power_locations.rename(columns={'Lat': 'lat', 'Lon': 'lon'}, inplace=True)\n",
    "\n",
    "    # need to use lat and lon to figure out the nearest bus - then add column called bus\n",
    "    df = df_wave_power_locations.rename(columns={'lat': 'y', 'lon': 'x'})\n",
    "    buses = dc.map_to_bus(df)\n",
    "    df_wave_power_locations['bus'] = buses\n",
    "\n",
    "    if year < 2025:\n",
    "        year = 2025\n",
    "    if fes == 2021:\n",
    "        date = str(year) + '-01-01'\n",
    "    else:\n",
    "        date = year\n",
    "    dic = {'capacities': df_wave_power_capacities.loc[date, :], 'locations': df_wave_power_locations}\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_marine_generators(year, scenario, fes):\n",
    "    # ADD NEW GENERATORS FOR MARINE\n",
    "\n",
    "    # get generators\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    df_LOPF = pd.read_csv(path, index_col=0)\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    df_UC = pd.read_csv(path_UC, index_col=0)\n",
    "\n",
    "    # want to remove historical marine generators\n",
    "    tech1 = 'Shoreline Wave'\n",
    "    tech2 = 'Tidal Barrage and Tidal Stream'\n",
    "    df_LOPF = df_LOPF[~df_LOPF.carrier.str.contains(tech1)]\n",
    "    df_LOPF = df_LOPF[~df_LOPF.carrier.str.contains(tech2)]\n",
    "    df_UC = df_UC[~df_UC.carrier.str.contains(tech1)]\n",
    "    df_UC = df_UC[~df_UC.carrier.str.contains(tech2)]\n",
    "\n",
    "    # read marine generators\n",
    "    read_tidal_lagoon_ = read_tidal_lagoon(year, scenario, fes)\n",
    "\n",
    "    df_tidal_lagoon = pd.DataFrame(read_tidal_lagoon_['capacities'])\n",
    "    df_tidal_lagoon.columns = ['p_nom']\n",
    "    # GW to MW\n",
    "    df_tidal_lagoon.loc[:, 'p_nom'] *= 1000\n",
    "    df_tidal_lagoon.index.name = 'name'\n",
    "    df_tidal_lagoon['carrier'] = 'Tidal lagoon'\n",
    "    df_tidal_lagoon['type'] = 'Tidal lagoon'\n",
    "    df_tidal_lagoon['bus'] = read_tidal_lagoon_['locations']['bus']\n",
    "    df_tidal_lagoon['marginal_cost'] = 0.0\n",
    "    df_tidal_lagoon['ramp_limit_up'] = 1.0\n",
    "    df_tidal_lagoon['ramp_limit_down'] = 1.0\n",
    "\n",
    "    read_tidal_stream_ = read_tidal_stream(year, scenario, fes)\n",
    "\n",
    "    df_tidal_stream = pd.DataFrame(read_tidal_stream_['capacities'])\n",
    "    df_tidal_stream.columns = ['p_nom']\n",
    "    df_tidal_stream.loc[:, 'p_nom'] *= 1000\n",
    "    df_tidal_stream.index.name = 'name'\n",
    "    df_tidal_stream['carrier'] = 'Tidal stream'\n",
    "    df_tidal_stream['type'] = 'Tidal stream'\n",
    "    df_tidal_stream['bus'] = read_tidal_stream_['locations']['bus']\n",
    "    df_tidal_stream['marginal_cost'] = 0.0\n",
    "    df_tidal_stream['ramp_limit_up'] = 1.0\n",
    "    df_tidal_stream['ramp_limit_down'] = 1.0\n",
    "\n",
    "    read_wave_power_ = read_wave_power(year, scenario, fes)\n",
    "\n",
    "    df_wave_power = pd.DataFrame(read_wave_power_['capacities'])\n",
    "    df_wave_power.columns = ['p_nom']\n",
    "    df_wave_power.loc[:, 'p_nom'] *= 1000\n",
    "    df_wave_power.index.name = 'name'\n",
    "    df_wave_power['carrier'] = 'Wave power'\n",
    "    df_wave_power['type'] = 'Wave power'\n",
    "    df_wave_power['bus'] = read_wave_power_['locations']['bus']\n",
    "    df_wave_power['marginal_cost'] = 0.0\n",
    "    df_wave_power['ramp_limit_up'] = 1.0\n",
    "    df_wave_power['ramp_limit_down'] = 1.0\n",
    "\n",
    "    # in shape to add to LOPF generators\n",
    "    df_LOPF = pd.concat([df_LOPF, df_tidal_lagoon, df_tidal_stream, df_wave_power])\n",
    "\n",
    "    df_LOPF.to_csv('LOPF_data/generators.csv', header=True)\n",
    "\n",
    "    # additional params for UC problem\n",
    "    df_tidal_lagoon['committable'] = False\n",
    "    df_tidal_lagoon['min_up_time'] = 0\n",
    "    df_tidal_lagoon['min_down_time'] = 0\n",
    "    df_tidal_lagoon['p_min_pu'] = 0\n",
    "    df_tidal_lagoon['up_time_before'] = 0\n",
    "    df_tidal_lagoon['start_up_cost'] = 0\n",
    "\n",
    "    df_tidal_stream['committable'] = False\n",
    "    df_tidal_stream['min_up_time'] = 0\n",
    "    df_tidal_stream['min_down_time'] = 0\n",
    "    df_tidal_stream['p_min_pu'] = 0\n",
    "    df_tidal_stream['up_time_before'] = 0\n",
    "    df_tidal_stream['start_up_cost'] = 0\n",
    "\n",
    "    df_wave_power['committable'] = False\n",
    "    df_wave_power['min_up_time'] = 0\n",
    "    df_wave_power['min_down_time'] = 0\n",
    "    df_wave_power['p_min_pu'] = 0\n",
    "    df_wave_power['up_time_before'] = 0\n",
    "    df_wave_power['start_up_cost'] = 0\n",
    "\n",
    "    # in shape to add to UC generators\n",
    "    df_UC = pd.concat([df_UC, df_tidal_lagoon, df_tidal_stream, df_wave_power])\n",
    "\n",
    "    df_UC.bus = 'bus'\n",
    "    df_UC.to_csv('UC_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5196da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_marine_timeseries(year, year_baseline, scenario, time_step):\n",
    "\n",
    "    path = 'LOPF_data/generators-p_max_pu.csv'\n",
    "    df_LOPF = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    year_orig = year\n",
    "\n",
    "    # in 5 year increments\n",
    "    if year < 2030:\n",
    "        year = 2025\n",
    "    elif year >= 2030 and year < 2035:\n",
    "        year = 2030\n",
    "    elif year >= 2035 and year < 2040:\n",
    "        year = 2035\n",
    "    elif year >= 2040 and year < 2045:\n",
    "        year = 2040\n",
    "    elif year >= 2045 and year < 2050:\n",
    "        year = 2045\n",
    "    elif year >= 2050:\n",
    "        year = 2050\n",
    "\n",
    "    # for interpolating\n",
    "    if time_step == 0.5:\n",
    "        freq = '0.5h'\n",
    "    elif time_step == 1:\n",
    "        freq = 'h'\n",
    "\n",
    "    # TIDAL LAGOON\n",
    "\n",
    "    df_tidal_lagoon = pd.read_excel('../data/renewables/Marine/tidal_lagoon_full.xlsx', sheet_name=str(year))\n",
    "    df_tidal_lagoon.index = df_tidal_lagoon['Date/time']\n",
    "    df_tidal_lagoon.drop(['Date/time'], axis=1, inplace=True)\n",
    "    df_tidal_lagoon.index = pd.to_datetime(df_tidal_lagoon.index)\n",
    "    df_tidal_lagoon.index = df_tidal_lagoon.index.round('H')\n",
    "    df_tidal_lagoon.drop(df_tidal_lagoon.tail(1).index, inplace=True)\n",
    "    df_tidal_lagoon.dropna(axis='columns', inplace=True)\n",
    "\n",
    "    # FUTURE WORK, NEED TO INCLUDE NEGATIVE VALUES AS A LOAD, BUT DROPPING FOR NOW\n",
    "\n",
    "    # fix inconsistent name\n",
    "    df_tidal_lagoon.rename(columns={'Colwyn Bay': 'Colwyn'}, inplace=True)\n",
    "    # interpolate to correct timestep\n",
    "    df_tidal_lagoon = df_tidal_lagoon.resample(freq).interpolate('polynomial', order=2)\n",
    "\n",
    "    if len(df_tidal_lagoon.index) < len(df_LOPF.index):\n",
    "\n",
    "        # add end value\n",
    "        end = df_LOPF.index.values[-1]\n",
    "        df_new_tidal_lagoon = pd.DataFrame(\n",
    "            data=df_tidal_lagoon.tail(1).values,\n",
    "            columns=df_tidal_lagoon.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_tidal_lagoon = pd.concat([df_tidal_lagoon, df_new_tidal_lagoon], sort=False)\n",
    "\n",
    "\n",
    "    df_tidal_lagoon[df_tidal_lagoon < 0] = 0\n",
    "    df_tidal_lagoon[df_tidal_lagoon > 1] = 1\n",
    "\n",
    "    df_tidal_lagoon.index = pd.to_datetime(df_tidal_lagoon.index)\n",
    "    # change the year to the df_LOPF year\n",
    "    df_tidal_lagoon.index = df_tidal_lagoon.index.map(lambda x : x.replace(year=year_orig))\n",
    "    df_LOPF.index = pd.to_datetime(df_LOPF.index)\n",
    "\n",
    "    # pick out required timeseries\n",
    "    df_tidal_lagoon = df_tidal_lagoon.loc[df_LOPF.index.values]\n",
    "    df_tidal_lagoon.index = df_LOPF.index\n",
    "\n",
    "    # TIDAL STREAM\n",
    "\n",
    "    path = '../data/renewables/Marine/tidal_stream_' + str(year) + '_full.xlsx'\n",
    "    df_tidal_stream = pd.read_excel(path)\n",
    "    df_tidal_stream.index = df_tidal_stream['Date/time']\n",
    "    df_tidal_stream.drop(['Date/time'], axis=1, inplace=True)\n",
    "    df_tidal_stream.index = pd.to_datetime(df_tidal_stream.index)\n",
    "    df_tidal_stream.index = df_tidal_stream.index.round('H')\n",
    "    df_tidal_stream.drop(df_tidal_stream.tail(1).index, inplace=True)\n",
    "    # interpolate to correct timestep\n",
    "    df_tidal_stream = df_tidal_stream.resample(freq).interpolate('polynomial', order=2)\n",
    "\n",
    "    if len(df_tidal_stream.index) < len(df_LOPF.index):\n",
    "\n",
    "        # add end value\n",
    "        end = df_LOPF.index.values[-1]\n",
    "        df_new_tidal_stream = pd.DataFrame(\n",
    "            data=df_tidal_stream.tail(1).values,\n",
    "            columns=df_tidal_stream.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_tidal_stream = pd.concat([df_tidal_stream, df_new_tidal_stream], sort=False)\n",
    "\n",
    "\n",
    "    df_tidal_stream[df_tidal_stream < 0] = 0\n",
    "    df_tidal_stream[df_tidal_stream > 1] = 1\n",
    "\n",
    "    df_tidal_stream.index = pd.to_datetime(df_tidal_stream.index)\n",
    "    df_LOPF.index = pd.to_datetime(df_LOPF.index)\n",
    "\n",
    "    # pick out required timeseries\n",
    "    df_tidal_stream.index = df_tidal_stream.index.map(lambda x : x.replace(year=year_orig))\n",
    "    df_tidal_stream = df_tidal_stream.loc[df_LOPF.index.values]\n",
    "    df_tidal_stream.index = df_LOPF.index\n",
    "\n",
    "    # WAVE POWER\n",
    "\n",
    "    df_wave_power = pd.read_csv('../data/renewables/Marine/capacity_factors_wave_full - Open Source.csv', index_col=0)\n",
    "    df_wave_power.index = pd.to_datetime(df_wave_power.index, format='%d/%m/%Y %H:%M')\n",
    "    # df_wave_power.index = df_wave_power.index.round('H')\n",
    "    # interpolate to correct timestep\n",
    "    df_wave_power = df_wave_power.resample(freq).interpolate('linear').round(5)\n",
    "\n",
    "    if len(df_wave_power.index) < len(df_LOPF.index):\n",
    "\n",
    "        # add end value\n",
    "        end = df_LOPF.index.values[-1]\n",
    "        df_new_wave_power = pd.DataFrame(\n",
    "            data=df_wave_power.tail(1).values,\n",
    "            columns=df_wave_power.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_wave_power = pd.concat([df_wave_power, df_new_wave_power], sort=False)\n",
    "\n",
    "\n",
    "    df_wave_power[df_wave_power < 0] = 0\n",
    "    df_wave_power[df_wave_power > 1] = 1\n",
    "\n",
    "    df_wave_power.index = pd.to_datetime(df_wave_power.index)\n",
    "    df_LOPF.index = pd.to_datetime(df_LOPF.index)\n",
    "\n",
    "    # pick out required timeseries\n",
    "    period = df_LOPF.index\n",
    "    period = pd.to_datetime(period)\n",
    "    # change year to baseline year\n",
    "    period = period.map(lambda t: t.replace(year=year_baseline))\n",
    "    df_wave_power = df_wave_power.loc[period.values]\n",
    "    df_wave_power.index = df_LOPF.index\n",
    "\n",
    "    # concat the DFs together\n",
    "    df_LOPF = pd.concat([df_LOPF, df_tidal_lagoon, df_tidal_stream, df_wave_power], axis=1)\n",
    "\n",
    "    # want to ensure no duplicate names\n",
    "    cols = pd.Series(df_LOPF.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    # rename the columns with the cols list.\n",
    "    df_LOPF.columns = cols\n",
    "    # make sure there are no missing values\n",
    "    df_LOPF = df_LOPF.fillna(0)\n",
    "    # make sure there are no negative values\n",
    "    df_LOPF[df_LOPF < 0] = 0\n",
    "    # fix the column names\n",
    "    df_LOPF.columns = df_LOPF.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df_LOPF.columns = df_LOPF.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "    df_LOPF.columns = df_LOPF.columns.astype(str).str.replace('ì', 'i')\n",
    "    df_LOPF.columns = df_LOPF.columns.str.strip()\n",
    "\n",
    "    df_LOPF.to_csv('LOPF_data/generators-p_max_pu.csv', header=True)\n",
    "    df_LOPF.to_csv('UC_data/generators-p_max_pu.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6691e37",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def aggregate_renewable_generation(start, end, year, time_step):\n",
    "\n",
    "    # want to aggregate renewable generation to speed up UC solving\n",
    "\n",
    "    # read in the generator file\n",
    "    df = pd.read_csv('UC_data/generators.csv')\n",
    "    # print(df)\n",
    "\n",
    "    freq = snapshots.write_snapshots(start, end, time_step)\n",
    "\n",
    "    # PV\n",
    "    df_PV = df.loc[df['carrier'] == 'Solar Photovoltaics'].reset_index(drop=True)\n",
    "    # delete PV from original dataframe\n",
    "    df = df[df.carrier != 'Solar Photovoltaics']\n",
    "    # add in row of total PV power\n",
    "    df = df.append({'name': 'PV',\n",
    "                    'carrier': 'Solar Photovoltaics',\n",
    "                    'type': 'Solar Photovoltaics',\n",
    "                    'p_nom': df_PV['p_nom'].sum(),\n",
    "                    'bus': 'bus',\n",
    "                    'marginal_cost': df_PV['marginal_cost'].mean(),\n",
    "                    'committable': df_PV['committable'][0],\n",
    "                    'min_up_time': df_PV['min_up_time'][0],\n",
    "                    'min_down_time': df_PV['min_down_time'][0],\n",
    "                    'ramp_limit_up': df_PV['ramp_limit_up'][0],\n",
    "                    'ramp_limit_down': df_PV['ramp_limit_down'][0],\n",
    "                    'up_time_before': df_PV['up_time_before'][0],\n",
    "                    'p_min_pu': df_PV['p_min_pu'][0],\n",
    "                    'start_up_cost': df_PV['start_up_cost'][0]}, ignore_index=True)\n",
    "    # read in the PV time series\n",
    "    df_PV_series = renewables_ninja_data_analysis.PV_corrected_series(year)\n",
    "    df_PV_series = df_PV_series.loc[start:end]\n",
    "\n",
    "    df_PV_aggregated_norm = df_PV_series.sum(axis=1) / 1000. / df_PV['p_nom'].sum()\n",
    "    df_PV_aggregated_norm = pd.DataFrame(df_PV_aggregated_norm, columns=['PV'])\n",
    "    # resample to half hourly timesteps\n",
    "    df_PV_aggregated_norm = df_PV_aggregated_norm.resample(freq).interpolate('polynomial', order=1)\n",
    "    # need to add a row at end\n",
    "    # the data being passed is the values of the last row\n",
    "    # the tail function is used to get the last index value\n",
    "    df_new_PV = pd.DataFrame(\n",
    "        data=[df_PV_aggregated_norm.loc[df_PV_aggregated_norm.tail(1).index.values].values[0]],\n",
    "        columns=df_PV_aggregated_norm.columns,\n",
    "        index=[end])\n",
    "    # add to existing dataframe\n",
    "    df_PV_aggregated_norm = pd.concat([df_PV_aggregated_norm, df_new_PV], sort=False)\n",
    "\n",
    "    df_PV_aggregated_norm.index.name = 'name'\n",
    "\n",
    "    # ONSHORE WIND\n",
    "    df_onshore = df.loc[df['carrier'] == 'Wind Onshore'].reset_index(drop=True)\n",
    "    # delete PV from original dataframe\n",
    "    df = df[df.carrier != 'Wind Onshore']\n",
    "    # add in row of total PV power\n",
    "    df = df.append({'name': 'wind_onshore',\n",
    "                    'carrier': 'Wind Onshore',\n",
    "                    'type': 'Wind Onshore',\n",
    "                    'p_nom': df_onshore['p_nom'].sum(),\n",
    "                    'bus': 'bus',\n",
    "                    'marginal_cost': df_onshore['marginal_cost'].mean(),\n",
    "                    'committable': df_onshore['committable'][0],\n",
    "                    'min_up_time': df_onshore['min_up_time'][0],\n",
    "                    'min_down_time': df_onshore['min_down_time'][0],\n",
    "                    'ramp_limit_up': df_onshore['ramp_limit_up'][0],\n",
    "                    'ramp_limit_down': df_onshore['ramp_limit_down'][0],\n",
    "                    'up_time_before': df_onshore['up_time_before'][0],\n",
    "                    'p_min_pu': df_onshore['p_min_pu'][0],\n",
    "                    'start_up_cost': df_onshore['start_up_cost'][0]}, ignore_index=True)\n",
    "    # read in the onshore wind time series\n",
    "    df_onshore_series = renewables_ninja_data_analysis.wind_onshore_corrected_series(year)\n",
    "\n",
    "    df_onshore_series = df_onshore_series.loc[start:end]\n",
    "\n",
    "    df_onshore_aggregated_norm = df_onshore_series.sum(axis=1) / 1000. / df_onshore['p_nom'].sum()\n",
    "    df_onshore_aggregated_norm = pd.DataFrame(df_onshore_aggregated_norm, columns=['wind_onshore'])\n",
    "\n",
    "    # resample to half hourly timesteps\n",
    "    df_onshore_aggregated_norm = df_onshore_aggregated_norm.resample(freq).interpolate('polynomial', order=1)\n",
    "    # need to add a row at end\n",
    "    # the data being passed is the values of the last row\n",
    "    # the tail function is used to get the last index value\n",
    "    df_new_offshore = pd.DataFrame(\n",
    "        data=[df_onshore_aggregated_norm.loc[df_onshore_aggregated_norm.tail(1).index.values].values[0]],\n",
    "        columns=df_onshore_aggregated_norm.columns,\n",
    "        index=[end])\n",
    "    # add to existing dataframe\n",
    "    df_onshore_aggregated_norm = pd.concat([df_onshore_aggregated_norm, df_new_offshore], ignore_index=True)\n",
    "\n",
    "    df_onshore_aggregated_norm.index.name = 'name'\n",
    "\n",
    "    # OFFSHORE WIND\n",
    "    df_offshore = df.loc[df['carrier'] == 'Wind Offshore'].reset_index(drop=True)\n",
    "    # delete PV from original dataframe\n",
    "    df = df[df.carrier != 'Wind Offshore']\n",
    "    # add in row of total PV power\n",
    "    df = df.append({'name': 'wind_offshore',\n",
    "                    'carrier': 'Wind Offshore',\n",
    "                    'type': 'Wind Offshore',\n",
    "                    'p_nom': df_offshore['p_nom'].sum(),\n",
    "                    'bus': 'bus',\n",
    "                    'marginal_cost': df_offshore['marginal_cost'].mean(),\n",
    "                    'committable': df_offshore['committable'][0],\n",
    "                    'min_up_time': df_offshore['min_up_time'][0],\n",
    "                    'min_down_time': df_offshore['min_down_time'][0],\n",
    "                    'ramp_limit_up': df_offshore['ramp_limit_up'][0],\n",
    "                    'ramp_limit_down': df_offshore['ramp_limit_down'][0],\n",
    "                    'up_time_before': df_offshore['up_time_before'][0],\n",
    "                    'p_min_pu': df_offshore['p_min_pu'][0],\n",
    "                    'start_up_cost': df_offshore['start_up_cost'][0]}, ignore_index=True)\n",
    "    # print(df)\n",
    "    # read in the onshore wind time series\n",
    "    df_offshore_series = renewables_ninja_data_analysis.wind_offshore_corrected_series(year)\n",
    "    df_offshore_series = df_offshore_series.loc[start:end]\n",
    "\n",
    "    df_offshore_aggregated_norm = df_offshore_series.sum(axis=1) / 1000. / df_offshore['p_nom'].sum()\n",
    "    df_offshore_aggregated_norm = pd.DataFrame(df_offshore_aggregated_norm, columns=['wind_offshore'])\n",
    "    # resample to half hourly timesteps\n",
    "    df_offshore_aggregated_norm = df_offshore_aggregated_norm.resample(freq).interpolate('polynomial', order=1)\n",
    "    # need to add a row at end\n",
    "    # the data being passed is the values of the last row\n",
    "    # the tail function is used to get the last index value\n",
    "    df_new_offshore = pd.DataFrame(\n",
    "        data=[df_offshore_aggregated_norm.loc[df_offshore_aggregated_norm.tail(1).index.values].values[0]],\n",
    "        columns=df_offshore_aggregated_norm.columns,\n",
    "        index=[end])\n",
    "    # add to existing dataframe\n",
    "    df_offshore_aggregated_norm = pd.concat([df_offshore_aggregated_norm, df_new_offshore], sort=False)\n",
    "\n",
    "    df_offshore_aggregated_norm.index.name = 'name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59c73f",
   "metadata": {},
   "source": [
    "    # FLOATING WIND\n",
    "    df_floating = df.loc[df['carrier'] == 'Floating Wind'].reset_index(drop=True)\n",
    "    # delete PV from original dataframe\n",
    "    df = df[df.carrier != 'Floating Wind']\n",
    "    # add in row of total PV power\n",
    "    df = df.append({'name': 'floating_wind',\n",
    "                    'carrier': 'Floating Wind',\n",
    "                    'type': 'Floating Wind',\n",
    "                    'p_nom': df_floating['p_nom'].sum(),\n",
    "                    'bus': 'bus',\n",
    "                    'marginal_cost': df_floating['marginal_cost'].mean(),\n",
    "                    'committable': df_floating['committable'][0],\n",
    "                    'min_up_time': df_floating['min_up_time'][0],\n",
    "                    'min_down_time': df_floating['min_down_time'][0],\n",
    "                    'ramp_limit_up': df_floating['ramp_limit_up'][0],\n",
    "                    'ramp_limit_down': df_floating['ramp_limit_down'][0],\n",
    "                    'up_time_before': df_floating['up_time_before'][0],\n",
    "                    'p_min_pu': df_floating['p_min_pu'][0],\n",
    "                    'start_up_cost': df_floating['start_up_cost'][0]}, ignore_index=True)\n",
    "    # print(df)\n",
    "    # read in the floating wind time series\n",
    "    df_floating_series = renewables_ninja_data_analysis.floating_wind_corrected_series(year)\n",
    "    df_floating_series = df_floating_series.loc[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d799c",
   "metadata": {},
   "source": [
    "    df_floating_aggregated_norm = df_floating_series.sum(axis=1) / 1000. / df_floating['p_nom'].sum()\n",
    "    df_floating_aggregated_norm = pd.DataFrame(df_floating_aggregated_norm, columns=['floating_wind'])\n",
    "    # resample to half hourly timesteps\n",
    "    df_floating_aggregated_norm = df_floating_aggregated_norm.resample(freq).interpolate('polynomial', order=1)\n",
    "    # need to add a row at end\n",
    "    # the data being passed is the values of the last row\n",
    "    # the tail function is used to get the last index value\n",
    "    df_new_floating = pd.DataFrame(\n",
    "        data=[df_floating_aggregated_norm.loc[df_floating_aggregated_norm.tail(1).index.values].values[0]],\n",
    "        columns=df_floating_aggregated_norm.columns,\n",
    "        index=[end])\n",
    "    # add to existing dataframe\n",
    "    df_floating_aggregated_norm = df_floating_aggregated_norm.append(df_new_floating, sort=False)\n",
    "    df_floating_aggregated_norm.index.name = 'name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc732a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # HYDRO\n",
    "\n",
    "    df_hydro_small = df.loc[df['carrier'] == 'Small Hydro'].reset_index(drop=True)\n",
    "    df_hydro_large = df.loc[df['carrier'] == 'Large Hydro'].reset_index(drop=True)\n",
    "    p_nom = df_hydro_small['p_nom'].sum() + df_hydro_large['p_nom'].sum()\n",
    "    # delete PV from original dataframe\n",
    "    df = df[df.carrier != 'Small Hydro']\n",
    "    df = df[df.carrier != 'Large Hydro']\n",
    "    # add in row of total PV power\n",
    "    df = df.append({'name': 'hydro',\n",
    "                    'carrier': 'Large Hydro',\n",
    "                    'type': 'Large Hydro',\n",
    "                    'p_nom': p_nom,\n",
    "                    'bus': 'bus',\n",
    "                    'marginal_cost': df_hydro_large['marginal_cost'].mean(),\n",
    "                    'committable': df_hydro_large['committable'][0],\n",
    "                    'min_up_time': df_hydro_large['min_up_time'][0],\n",
    "                    'min_down_time': df_hydro_large['min_down_time'][0],\n",
    "                    'ramp_limit_up': df_hydro_large['ramp_limit_up'][0],\n",
    "                    'ramp_limit_down': df_hydro_large['ramp_limit_down'][0],\n",
    "                    'up_time_before': df_hydro_large['up_time_before'][0],\n",
    "                    'p_min_pu': df_hydro_large['p_min_pu'][0],\n",
    "                    'start_up_cost': df_hydro_large['start_up_cost'][0]}, ignore_index=True)\n",
    "\n",
    "    df_series = pd.read_csv('UC_data/generators-p_max_pu.csv')\n",
    "    # print(df_series)\n",
    "    # limited to using bonnington which has been there for years so should work,\n",
    "    # but this could be improved\n",
    "    df_hydro_aggregated_norm = df_series['Bonnington'].values[:len(df_offshore_aggregated_norm.index)]\n",
    "    df_hydro_aggregated_norm = pd.DataFrame(\n",
    "        df_hydro_aggregated_norm, columns=['hydro'], index=df_offshore_aggregated_norm.index)\n",
    "\n",
    "    # concat the time series for the RES tech\n",
    "    df_res = pd.concat(\n",
    "        [df_offshore_aggregated_norm, df_onshore_aggregated_norm,\n",
    "         df_PV_aggregated_norm, df_hydro_aggregated_norm], axis=1)\n",
    "    # print(df_res)\n",
    "    # df_res = df_res.loc[start:end]\n",
    "    # print(df_res)\n",
    "\n",
    "    # save the new generators and generators p max pu files\n",
    "    df_res.to_csv('UC_data/generators-p_max_pu.csv', header=True)\n",
    "    df.to_csv('UC_data/generators.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RES_correction_factors():\n",
    "    \"\"\"correction factor to match annual model and actual output for RES generation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        correction factor for all tech and years\n",
    "    \"\"\"\n",
    "\n",
    "    offshore_data = {'2010': 3060,\n",
    "                     '2011': 5149,\n",
    "                     '2012': 7603,\n",
    "                     '2013': 11472,\n",
    "                     '2014': 13405,\n",
    "                     '2015': 17423,\n",
    "                     '2016': 16406,\n",
    "                     '2017': 20916,\n",
    "                     '2018': 26525,\n",
    "                     '2019': 31975,\n",
    "                     '2020': 40681}\n",
    "\n",
    "    onshore_data = {'2010': 7226,\n",
    "                    '2011': 10814,\n",
    "                    '2012': 12244,\n",
    "                    '2013': 16925,\n",
    "                    '2014': 18555,\n",
    "                    '2015': 22852,\n",
    "                    '2016': 20754,\n",
    "                    '2017': 28725,\n",
    "                    '2018': 30382,\n",
    "                    '2019': 31820,\n",
    "                    '2020': 34688}\n",
    "\n",
    "    PV_data = {'2010': 40,\n",
    "               '2011': 244,\n",
    "               '2012': 1354,\n",
    "               '2013': 2010,\n",
    "               '2014': 4054,\n",
    "               '2015': 7533,\n",
    "               '2016': 10398,\n",
    "               '2017': 11457,\n",
    "               '2018': 12668,\n",
    "               '2019': 12580,\n",
    "               '2020': 13158}\n",
    "\n",
    "    gen_data_dict = {'Wind_Offshore': offshore_data,\n",
    "                     'Wind_Onshore': onshore_data,\n",
    "                     'PV': PV_data}\n",
    "    years = list(range(2010, 2020 + 1))\n",
    "    tech = ['Wind_Offshore', 'Wind_Onshore', 'PV']\n",
    "    factor_dict = {}\n",
    "    # tech = ['PV']\n",
    "    for y in years:\n",
    "        factor_dict_year = {}\n",
    "        for t in tech:\n",
    "            if y == 2010 and t == 'PV':\n",
    "                factor_dict_year[t] = np.nan\n",
    "            else:\n",
    "                gen_year = gen_data_dict[t][str(y)]\n",
    "                path = '../data/renewables/atlite/outputs/' + t + '/' + t + '_' + str(y) + '.csv'\n",
    "                t_ = t.replace(\"_\", \" \")\n",
    "                if t == 'PV':\n",
    "                    t_ = 'Solar Photovoltaics'\n",
    "                df = fix_timeseries_res_for_year(path, y, t_, future=False)\n",
    "                modelled_generation = df.sum().sum() / 1000\n",
    "                factor = gen_year / modelled_generation\n",
    "                factor_dict_year[t] = factor\n",
    "        factor_dict[y] = factor_dict_year\n",
    "\n",
    "    path = '../data/renewables/atlite/'\n",
    "    file = 'RES_correction_factors.csv'\n",
    "    df_factors = pd.DataFrame(factor_dict)\n",
    "    df_factors.to_csv(path + file, header=True)\n",
    "\n",
    "    return df_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0485e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_RES_timeseries(year, tech, future=False):\n",
    "\n",
    "    tech_ = tech.replace(\" \", \"_\")\n",
    "    if tech == 'Solar Photovoltaics':\n",
    "        tech_ = 'PV'\n",
    "    path = '../data/renewables/atlite/outputs/' + tech_ + '/' + tech_ + '_' + str(year) + '.csv'\n",
    "    # this returns the atlite time series, but corrected by date of operation\n",
    "    df = fix_timeseries_res_for_year(path, year, tech, future=future)\n",
    "    # fix the column names\n",
    "    df.columns = df.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df.columns = df.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "    df.columns = df.columns.astype(str).str.replace('ì', 'i')\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # # now want to normalise using capacities...\n",
    "    # # read in the renewable generators\n",
    "    df_gen = pd.read_csv('LOPF_data/generators.csv', index_col=0)\n",
    "    df_res_tech = df_gen.loc[df_gen['carrier'] == tech]\n",
    "\n",
    "    # need to remove future and pipeline wind offshore\n",
    "    if tech == 'Wind Offshore' and future is True:\n",
    "        # get index of future and pipeline names\n",
    "        # then get pipeline timeseries\n",
    "        path = '../data/renewables/atlite/outputs/Wind_Offshore/wind_offshore_pipeline/'\n",
    "        file = 'wind_offshore_pipeline_' + str(2020) + '.csv'  # year dosent matter\n",
    "        df_pipeline = pd.read_csv(path + file, index_col=0)\n",
    "        # fix the column names\n",
    "        df_pipeline.columns = df_pipeline.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "        df_pipeline.columns = df_pipeline.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "        df_pipeline.columns = df_pipeline.columns.astype(str).str.replace('ì', 'i')\n",
    "        df_pipeline.columns = df_pipeline.columns.str.strip()\n",
    "\n",
    "        # first get the timeseries for these areas\n",
    "        path = '../data/renewables/atlite/outputs/Wind_Offshore/wind_offshore_future/'\n",
    "        file = 'wind_offshore_future_' + str(2020) + '.csv'  # year dosent matter\n",
    "        df_future = pd.read_csv(path + file, index_col=0)\n",
    "        # fix the column names\n",
    "        df_future.columns = df_future.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "        df_future.columns = df_future.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "        df_future.columns = df_future.columns.astype(str).str.replace('ì', 'i')\n",
    "        df_future.columns = df_future.columns.str.strip()\n",
    "\n",
    "        # remove them from df_res_tech\n",
    "        df_res_tech.drop(df_pipeline.columns, inplace=True)\n",
    "        df_res_tech.drop(df_future.columns, inplace=True)\n",
    "\n",
    "    df_norm = df.copy()\n",
    "    for gen in df_norm.columns:\n",
    "        # print(gen)\n",
    "        # print(df_norm.columns)\n",
    "        p_max = df_norm.loc[:, gen].max()\n",
    "        df_norm.loc[:, gen] /= p_max\n",
    "\n",
    "    return_dict = {'timeseries': df, 'norm': df_norm}\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_RES_scale_p_nom(year, tech, scenario, FES):\n",
    "\n",
    "    future_capacities_dict = future_RES_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    if tech == 'Hydro':\n",
    "        t1 = 'Small Hydro'\n",
    "        t2 = 'Large Hydro'\n",
    "\n",
    "        # get generators dataframe with p_noms to be scaled\n",
    "        path = 'LOPF_data/generators.csv'\n",
    "        generators = pd.read_csv(path, index_col=0)\n",
    "        # this deletes the hydrogen generators so need to keep these\n",
    "        hydrogen = generators.loc[generators['carrier'] == 'Hydrogen']\n",
    "        gen_tech1 = generators.loc[generators['carrier'] == t1]\n",
    "        gen_tech2 = generators.loc[generators['carrier'] == t2]\n",
    "        gen_tech = pd.concat([gen_tech1, gen_tech2])\n",
    "\n",
    "        path_UC = 'UC_data/generators.csv'\n",
    "        generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "        # this deletes the hydrogen generators so need to keep these\n",
    "        hydrogen_UC = generators_UC.loc[generators_UC['carrier'] == 'Hydrogen']\n",
    "        gen_tech_UC1 = generators_UC.loc[generators_UC['carrier'] == t1]\n",
    "        gen_tech_UC2 = generators_UC.loc[generators_UC['carrier'] == t2]\n",
    "        gen_tech_UC = pd.concat([gen_tech_UC1, gen_tech_UC2])\n",
    "\n",
    "    elif tech == 'Wind Onshore' or tech == 'Solar Photovoltaics' or 'Wind Offshore':\n",
    "\n",
    "        # get generators dataframe with p_noms to be scaled\n",
    "        path = 'LOPF_data/generators.csv'\n",
    "        generators = pd.read_csv(path, index_col=0)\n",
    "        gen_tech = generators.loc[generators['carrier'] == tech]\n",
    "\n",
    "        path_UC = 'UC_data/generators.csv'\n",
    "        generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "        gen_tech_UC = generators_UC.loc[generators_UC['carrier'] == tech]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = tech_cap_FES / tech_cap_year\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to remove the original tech\n",
    "    # print(generators.type.to_list())\n",
    "    generators = generators[~generators.type.str.contains(tech)]\n",
    "    generators_UC = generators_UC[~generators_UC.type.str.contains(tech)]\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech])\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC])\n",
    "\n",
    "    if tech == 'Hydro':\n",
    "        generators = pd.concat([generators, hydrogen])\n",
    "        generators_UC = pd.concat([generators_UC, hydrogen_UC])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_offshore_timeseries(year, year_baseline, scenario, FES):\n",
    "\n",
    "    future_capacities_dict = future_offshore_capacity(year, year_baseline, scenario, FES)\n",
    "    # print(future_capacities_dict)\n",
    "    offshore_cap_year = future_capacities_dict['offshore_cap_year']\n",
    "    offshore_cap_pipeline = future_capacities_dict['offshore_cap_pipeline']\n",
    "    offshore_cap_scotland_planning = future_capacities_dict['offshore_cap_scotland_planning']\n",
    "    offshore_cap_FES = future_capacities_dict['offshore_cap_FES']\n",
    "\n",
    "    # first get timeseries for baseline year\n",
    "    tech = 'Wind Offshore'\n",
    "    df_baseline = historical_RES_timeseries(year_baseline, tech, future=True)['timeseries']\n",
    "\n",
    "    # then get pipeline timeseries\n",
    "    path = '../data/renewables/atlite/outputs/Wind_Offshore/wind_offshore_pipeline/'\n",
    "    file = 'wind_offshore_pipeline_' + str(year_baseline) + '.csv'\n",
    "    df_pipeline = pd.read_csv(path + file, index_col=0)\n",
    "    df_pipeline.index = df_baseline.index\n",
    "\n",
    "    # first get the timeseries for these areas\n",
    "    path = '../data/renewables/atlite/outputs/Wind_Offshore/wind_offshore_future/'\n",
    "    file = 'wind_offshore_future_' + str(year_baseline) + '.csv'\n",
    "    df_future = pd.read_csv(path + file, index_col=0)\n",
    "    df_future.index = df_baseline.index\n",
    "\n",
    "    # check if baseline year is a leap year and simulated year is not and remove 29th Feb\n",
    "    if year_baseline % 4 == 0:\n",
    "        # and the year modelled is also not a leap year\n",
    "        if year % 4 != 0:\n",
    "            # remove 29th Feb\n",
    "            df_baseline = df_baseline[~((df_baseline.index.month == 2) & (df_baseline.index.day == 29))]\n",
    "            df_pipeline = df_pipeline[~((df_pipeline.index.month == 2) & (df_pipeline.index.day == 29))]\n",
    "            df_future = df_future[~((df_future.index.month == 2) & (df_future.index.day == 29))]\n",
    "\n",
    "    # now want to normalise using capacities...\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['carrier'] == 'Wind Offshore']\n",
    "\n",
    "    # now want to normalise using capacities...\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['carrier'] == 'Wind Offshore']\n",
    "\n",
    "    # combine the timeseries\n",
    "    result = pd.concat([df_baseline, df_pipeline, df_future], axis=1)\n",
    "\n",
    "    # clean up strings in these original dataframes\n",
    "    df_baseline.columns = df_baseline.columns.astype(str).str.replace(u'\\xa0', ' ')\n",
    "    df_baseline = df_baseline.rename(columns=lambda x: x.strip())\n",
    "    df_baseline.columns = df_baseline.columns.astype(str).str.replace('ì', 'i')\n",
    "    df_pipeline.columns = df_pipeline.columns.astype(str).str.replace(u'\\xa0', ' ')\n",
    "    df_pipeline = df_pipeline.rename(columns=lambda x: x.strip())\n",
    "    df_pipeline.columns = df_pipeline.columns.astype(str).str.replace('ì', 'i')\n",
    "    df_future.columns = df_future.columns.astype(str).str.replace(u'\\xa0', ' ')\n",
    "    df_future = df_future.rename(columns=lambda x: x.strip())\n",
    "    df_future.columns = df_future.columns.astype(str).str.replace('ì', 'i')\n",
    "\n",
    "    # clean up the strings\n",
    "    result.columns = result.columns.astype(str).str.replace(u'\\xa0', ' ')\n",
    "    result.columns = result.columns.astype(str).str.replace('ì', 'i')\n",
    "    result = result.rename(columns=lambda x: x.strip())\n",
    "\n",
    "    gen_tech.index = gen_tech.index.str.strip()\n",
    "    gen_tech.index = gen_tech.index.astype(str).str.replace(u'\\xa0', ' ')\n",
    "    gen_tech_UC.index = gen_tech_UC.index.str.strip()\n",
    "    gen_tech_UC.index = gen_tech_UC.index.astype(str).str.replace(u'\\xa0', ' ')\n",
    "\n",
    "    # then normalise all the different units\n",
    "    result_norm = pd.DataFrame(result)\n",
    "    # print(result_norm.columns)\n",
    "    # print(gen_tech)\n",
    "    for gen in result_norm.columns:\n",
    "        p_max = result_norm.loc[:, gen].max()\n",
    "        result_norm.loc[:, gen] /= p_max\n",
    "\n",
    "    # change the year on the indexes to the year simulated\n",
    "    result.index = result.index + pd.DateOffset(year=year)\n",
    "    result_norm.index = result_norm.index + pd.DateOffset(year=year)\n",
    "\n",
    "    # now want to scale the p_nom of offshore units\n",
    "    # print(offshore_cap_FES, 'required')\n",
    "    # print(offshore_cap_year, '2020??')\n",
    "    # print(offshore_cap_pipeline, 'pipeline for year')\n",
    "    # print(gen_tech.p_nom.sum(), 'sum to begin with')\n",
    "    # then consider what capacity still needs to be built\n",
    "    cap_req = offshore_cap_FES - offshore_cap_year\n",
    "\n",
    "    if cap_req <= offshore_cap_pipeline:\n",
    "        # then pipeline available capacity should be scaled down\n",
    "        # or by 1 if it is exactly right\n",
    "        # scale by the total pipeline available as the dict value is the pipeline available in modelled year\n",
    "        # but need to scaled total pipeline\n",
    "        pipeline_factor = cap_req / 18.3059\n",
    "    elif cap_req > offshore_cap_pipeline:\n",
    "        # if still need more capacity just use the pipeline as is\n",
    "        pipeline_factor = offshore_cap_pipeline / 18.3059\n",
    "\n",
    "    # print(pipeline_factor, 'pipeline scaling factor')\n",
    "\n",
    "    # scale the p_noms down for pipeline wind turbines\n",
    "    for g in df_pipeline.columns:\n",
    "        gen_tech.loc[g, 'p_nom'] *= pipeline_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= pipeline_factor\n",
    "\n",
    "    # print(gen_tech.p_nom.sum(), 'sum after including pipeline')\n",
    "\n",
    "    # then check if still need more capacity built\n",
    "    cap_req = round(offshore_cap_FES - offshore_cap_year - offshore_cap_pipeline, 2)\n",
    "    # print(cap_req, 'cap required after pipeline')\n",
    "    # print(cap_req, 'capacity required over 2020 capacity + pipeline capacity')\n",
    "    # next step is to use capacity from Marine Sector Plan for Scotland\n",
    "\n",
    "    if cap_req <= 0:\n",
    "        # if not capacity required then zero\n",
    "        future_factor = 0\n",
    "    elif cap_req <= offshore_cap_scotland_planning:\n",
    "        # then future available capacity should be scaled down\n",
    "        # or by 1 if it is exactly right\n",
    "        future_factor = cap_req / offshore_cap_scotland_planning\n",
    "    elif cap_req > offshore_cap_pipeline:\n",
    "        # if still need more capacity just use the future timeseries as is\n",
    "        future_factor = 1\n",
    "    # print(future_factor, 'future planned scaling factor')\n",
    "\n",
    "    # scale the future wind offshore units\n",
    "    for g in df_future.columns:\n",
    "        gen_tech.loc[g, 'p_nom'] *= future_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= future_factor\n",
    "\n",
    "    # print(gen_tech.p_nom.sum(), 'sum after future sites included')\n",
    "\n",
    "    # now check if 2020 + pipeline + future sites is enough\n",
    "    cap_req = round(\n",
    "        offshore_cap_FES - offshore_cap_year - offshore_cap_pipeline - offshore_cap_scotland_planning, 2)\n",
    "    # print(cap_req, 'capacity required over 2020 capacity + pipeline capacity + planned capacity')\n",
    "    if cap_req > 0:\n",
    "        # final step is to linearly scale the combined timeseries to get to required capacity\n",
    "        scaling_factor = round(\n",
    "            offshore_cap_FES / (offshore_cap_year + offshore_cap_pipeline + offshore_cap_scotland_planning),\n",
    "            2)\n",
    "    else:\n",
    "        scaling_factor = 1\n",
    "    for g in result.columns:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    generators.loc[generators['carrier'] == 'Wind Offshore'] = gen_tech\n",
    "    generators_UC.loc[generators_UC['carrier'] == 'Wind Offshore'] = gen_tech_UC\n",
    "\n",
    "    # print(gen_tech.p_nom.sum(), 'final sum')\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)\n",
    "\n",
    "    return_dict = {'timeseries': result, 'norm': result_norm}\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68101344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_offshore_sites(year):\n",
    "    # how much wind in 2020\n",
    "    # read in the renewable generators\n",
    "    df_res = REPD_date_corrected(year)\n",
    "    # start with the offshore wind farms\n",
    "    df_res_offshore = df_res.loc[df_res['Technology Type'] == 'Wind Offshore'].reset_index(drop=True)\n",
    "    # installed capacity 2020\n",
    "    print(df_res_offshore['Installed Capacity (MWelec)'].sum() / 1000, 'GW installed 2020')\n",
    "\n",
    "    # pipeline data\n",
    "    df_pipeline = pd.read_csv('data/renewables/future_offshore_sites/offshore_pipeline.csv',\n",
    "                              encoding='unicode_escape', index_col=2)\n",
    "    df_pipeline.drop(columns=['Record Last Updated (dd/mm/yyyy)', 'Operator (or Applicant)',\n",
    "                              'Under Construction', 'Technology Type',\n",
    "                              'Planning Permission Expired', 'Operational',\n",
    "                              'Heat Network Ref', 'Planning Authority',\n",
    "                              'Planning Application Submitted', 'Region',\n",
    "                              'Country', 'County'], inplace=True)\n",
    "    df_pipeline.dropna(axis='columns', inplace=True)\n",
    "    # capacity in pipeline\n",
    "    # print(df_pipeline['Installed Capacity (MWelec)'].sum() / 1000, 'GW in pipeline')\n",
    "    # This takes us as far as 2027 for the Leading the Way scenario\n",
    "\n",
    "    # lets look at pipeline output for these years\n",
    "    df = df_pipeline['Expected Operational']\n",
    "    df = pd.to_datetime(df).dt.to_period('D')\n",
    "    # cut off is the end of the year being simulated\n",
    "    for y in [2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030]:\n",
    "        date = '31/12/' + str(y)\n",
    "        df2 = df_pipeline[~(df > date)]\n",
    "        new_gw = df2['Installed Capacity (MWelec)'].sum() / 1000\n",
    "        total_gw = new_gw + df_res_offshore['Installed Capacity (MWelec)'].sum() / 1000\n",
    "        # print(new_gw, 'New GW in ' + str(y))\n",
    "        # print(total_gw, 'Total GW in ' + str(y))\n",
    "\n",
    "    # pipeline + operational takes us to 25.4GW in 2027\n",
    "    # lets look at future fields and the potential capacity which exists there...\n",
    "\n",
    "    # max 26GW in Scottish waters... takes us to 51.4GW...\n",
    "    # this is exceeded in 2031 in leading the way scenario\n",
    "    # highest offshore wind capacity in FES in 113.2GW\n",
    "    # need to simply scale up distributions by assuming far offshore from 2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b49185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_offshore_capacity(year, year_baseline, scenario, FES):\n",
    "    # how much wind in baseline year\n",
    "    # read in the renewable generators\n",
    "    df_res = REPD_date_corrected(2020)\n",
    "    # start with the offshore wind farms\n",
    "    df_res_offshore = df_res.loc[df_res['Technology Type'] == 'Wind Offshore'].reset_index(drop=True)\n",
    "    # installed capacity in baseline year\n",
    "    offshore_cap_year = df_res_offshore['Installed Capacity (MWelec)'].sum() / 1000\n",
    "\n",
    "    # pipeline data\n",
    "    df_pipeline = pd.read_csv('../data/renewables/future_offshore_sites/offshore_pipeline.csv',\n",
    "                              encoding='unicode_escape', index_col=2)\n",
    "    df_pipeline.drop(columns=['Record Last Updated (dd/mm/yyyy)', 'Operator (or Applicant)',\n",
    "                              'Under Construction', 'Technology Type',\n",
    "                              'Planning Permission Expired', 'Operational',\n",
    "                              'Heat Network Ref', 'Planning Authority',\n",
    "                              'Planning Application Submitted', 'Region',\n",
    "                              'Country', 'County'], inplace=True)\n",
    "    df_pipeline.dropna(axis='columns', inplace=True)\n",
    "    # pipeline up to 2030, but still add in pipeline after 2030\n",
    "    if year > 2030:\n",
    "        year_pipeline = 2030\n",
    "    else:\n",
    "        year_pipeline = year\n",
    "    # lets look at pipeline output for these years\n",
    "    df = df_pipeline['Expected Operational']\n",
    "    df = pd.to_datetime(df).dt.to_period('D')\n",
    "\n",
    "    date = '31/12/' + str(year_pipeline)\n",
    "    df2 = df_pipeline[~(df > date)]\n",
    "    offshore_cap_pipeline = df2['Installed Capacity (MWelec)'].sum() / 1000\n",
    "    # print(offshore_cap_pipeline, 'New GW in ' + str(year_pipeline))\n",
    "\n",
    "    df_scotland = pd.read_csv('../data/renewables/future_offshore_sites/Sectoral Marine Plan 2020.csv',\n",
    "                              encoding='unicode_escape')\n",
    "    offshore_cap_scotland_planning = df_scotland['max capacity (GW)'].sum()\n",
    "\n",
    "    if FES == 2021:\n",
    "        # offshore wind capacity from FES2021\n",
    "        df_FES = pd.read_excel(\n",
    "            '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "            sheet_name='SV.28', usecols=\"M:AS\", header=7, dtype=str,\n",
    "            index_col=1)\n",
    "        df_FES.drop(columns=['Unnamed: 12'], inplace=True)\n",
    "        df_FES.dropna(axis='rows', inplace=True)\n",
    "        date = str(year) + '-01-01'\n",
    "        if scenario == 'Leading The Way':\n",
    "            scenario = 'Leading the Way'\n",
    "        offshore_cap_FES = float(df_FES.loc[scenario, date])\n",
    "        # print(type(offshore_cap_FES))\n",
    "    elif FES == 2022:\n",
    "        df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        # df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES = df_FES[df_FES.Type.str.contains('Offshore Wind', case=False)]\n",
    "        df_FES = df_FES[df_FES.Variable.str.contains('Capacity')]\n",
    "        df_FES = df_FES[~df_FES.Connection.str.contains('Non-Grid Connected')]\n",
    "\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "        df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "        df_FES_LTW.index = ['Leading The Way']\n",
    "\n",
    "        df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "        df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "        df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "        df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "        df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "        df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "        df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "        df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "        df_FES = df_FES / 1000.\n",
    "\n",
    "        offshore_cap_FES = df_FES.loc[scenario, year]\n",
    "\n",
    "    capacity_dict = {'offshore_cap_year': offshore_cap_year,\n",
    "                     'offshore_cap_pipeline': offshore_cap_pipeline,\n",
    "                     'offshore_cap_scotland_planning': offshore_cap_scotland_planning,\n",
    "                     'offshore_cap_FES': offshore_cap_FES}\n",
    "    # print(capacity_dict)\n",
    "    return capacity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_RES_capacity(year, tech, scenario, FES):\n",
    "\n",
    "    if tech == 'Hydro':\n",
    "        df_hydro = read_hydro(year)\n",
    "        # drop pumped hydro\n",
    "        df_hydro = df_hydro[~df_hydro.type.str.contains('Pumped Storage Hydroelectricity')]\n",
    "        tech_cap_year = df_hydro['p_nom'].sum() / 1000\n",
    "\n",
    "    elif tech == 'Wind Onshore' or tech == 'Solar Photovoltaics' or tech == 'Wind Offshore':\n",
    "        # how much RES in year\n",
    "        # read in the renewable generators\n",
    "        df_res = REPD_date_corrected(year)\n",
    "        # start with the offshore wind farms\n",
    "        df_res_tech = df_res.loc[df_res['Technology Type'] == tech].reset_index(drop=True)\n",
    "        # installed capacity year\n",
    "        tech_cap_year = df_res_tech['Installed Capacity (MWelec)'].sum() / 1000\n",
    "\n",
    "    elif tech == 'Biomass':\n",
    "        df_biomass = read_biomass(year)\n",
    "        tech_cap_year = df_biomass['p_nom'].sum() / 1000\n",
    "\n",
    "    # how much RES in year to be simulated\n",
    "    if tech == 'Wind Onshore':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='SV.29', usecols=\"M:AS\", header=6, dtype=str,\n",
    "                index_col=1)\n",
    "            df_FES.drop(columns=['Unnamed: 12'], inplace=True)\n",
    "            df_FES.dropna(axis='rows', inplace=True)\n",
    "        if FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            df_FES = df_FES[df_FES.SubType.str.contains('Onshore Wind', case=False, na=False)]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Curtailment')]\n",
    "            cols = [0, 1, 2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "            df_FES = df_FES / 1000\n",
    "\n",
    "    elif tech == 'Solar Photovoltaics':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='SV.31', usecols=\"L:AR\", header=5, dtype=str,\n",
    "                index_col=1)\n",
    "            df_FES.drop(columns=['Unnamed: 11'], inplace=True)\n",
    "            df_FES.dropna(axis='rows', inplace=True)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            df_FES = df_FES[df_FES.SubType.str.contains('Solar PV', case=False, na=False)]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            cols = [0, 1, 2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "            df_FES = df_FES / 1000\n",
    "\n",
    "    elif tech == 'Hydro':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            df_FES.dropna(axis='rows', inplace=True)\n",
    "            df_FES = df_FES[df_FES.Type.str.contains('Hydro', case=False)]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            # df_FES = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            cols = [2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            df_FES.dropna(axis='rows', inplace=True)\n",
    "            df_FES = df_FES[df_FES.SubType.str.contains('Hydro', case=False, na=False)]\n",
    "            # drop pumped hydro\n",
    "            df_FES = df_FES[~df_FES.SubType.str.contains('Pumped Hydro')]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            cols = [0, 1, 2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'Biomass':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            df_FES = df_FES[df_FES.Type.str.contains('Biomass', case=False, na=False)]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            cols = [2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "            # df_FES.dropna(axis='rows', inplace=True)\n",
    "            df_FES = df_FES[df_FES.SubType.str.contains('Biomass', case=False, na=False)]\n",
    "            df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "            df_FES = df_FES[~df_FES.SubType.str.contains('CCS Biomass')]\n",
    "            cols = [0, 1, 2, 3, 4]\n",
    "            df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "            df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "            df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    date = str(year) + '-01-01'\n",
    "    if scenario == 'Leading The Way':\n",
    "        scenario = 'Leading the Way'\n",
    "\n",
    "    if tech == 'Wind Onshore' or tech == 'Solar Photovoltaics':\n",
    "        try:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, date])\n",
    "        except:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, year])\n",
    "    else:\n",
    "        try:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, date]) / 1000.\n",
    "        except:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, year]) / 1000.\n",
    "\n",
    "    capacity_dict = {'tech_cap_year': tech_cap_year,\n",
    "                     'tech_cap_FES': tech_cap_FES}\n",
    "\n",
    "    return capacity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d125ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_future_capacities(year):\n",
    "\n",
    "    start = str(year) + '-12-02 00:00:00'\n",
    "    end = str(year) + '-12-02 03:30:00'\n",
    "    # time step as fraction of hour\n",
    "    time_step = 0.5\n",
    "    if year > 2020:\n",
    "        data_reader_writer.data_writer(start, end, time_step, year, year_baseline=2020, scenario='Leading The Way')\n",
    "    if year <= 2020:\n",
    "        data_reader_writer.data_writer(start, end, time_step, year)\n",
    "\n",
    "    df_generators = pd.read_csv('LOPF_data/generators.csv', index_col=0)\n",
    "    generators_p_nom = df_generators.p_nom.groupby(df_generators.carrier).sum()\n",
    "    generators_p_nom.drop('Unmet Load', inplace=True)\n",
    "    try:\n",
    "        generators_p_nom.drop('Coal', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    # generators_p_nom.drop(generators_p_nom[generators_p_nom < 500].index, inplace=True)\n",
    "\n",
    "    # bar chart\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    col_map = plt.get_cmap('Paired')\n",
    "    plt.bar(generators_p_nom.index, generators_p_nom.values / 1000, color=col_map.colors, edgecolor='k')\n",
    "    plt.xticks(generators_p_nom.index, rotation=90)\n",
    "    plt.ylabel('GW')\n",
    "    plt.grid(color='grey', linewidth=1, axis='both', alpha=0.5)\n",
    "    plt.title('Installed capacity in year ' + str(year))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/FES2021/Capacities Pics/' + str(year) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_future_capacities():\n",
    "\n",
    "    filenames = []\n",
    "    for year in range(2021, 2050 + 1):\n",
    "        plot_future_capacities(year)\n",
    "        # list of filenames\n",
    "        filenames.append('../data/FES2021/Capacities Pics/' + str(year) + '.png')\n",
    "\n",
    "    with imageio.get_writer('../data/FES2021/Capacities Pics/FES_installed_capacities.gif', mode='I', duration=1.) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54719964",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # year = 2020\n",
    "    # future_offshore_sites(year)\n",
    "\n",
    "    # year = 2027\n",
    "    # future_offshore_capacity(year)\n",
    "\n",
    "    # year = 2017\n",
    "    # tech = 'Solar Photovoltaics'\n",
    "    # historical_RES_timeseries(year, tech)['norm']\n",
    "    # historical_RES_timeseries(year, tech)['timeseries']\n",
    "\n",
    "    # RES_correction_factors()\n",
    "\n",
    "    # year = 2025\n",
    "    # future_offshore_capacity(year)\n",
    "\n",
    "    # year = 2050\n",
    "    # future_offshore_timeseries(year)\n",
    "    # output = future_offshore_timeseries(year)\n",
    "    # print(output['timeseries'])\n",
    "    # print(output['norm'])\n",
    "\n",
    "    # year = 2050\n",
    "    # tech = 'Wind Onshore'\n",
    "    # tech = 'Hydro'\n",
    "    # tech = 'Solar Photovoltaics'\n",
    "\n",
    "    # future_RES_capacity(year, tech)\n",
    "    # future_RES_scale_p_nom(year, tech)\n",
    "\n",
    "    # gif_future_capacities()\n",
    "\n",
    "    # scenario = 'Consumer Transformation'\n",
    "    # scenario = 'Leading The Way'\n",
    "    # scenario = 'System Transformation'\n",
    "    # scenario = 'Steady Progression'\n",
    "    # year = 2050\n",
    "    # print(read_tidal_lagoon(year, scenario))\n",
    "    # print(read_tidal_stream(year, scenario))\n",
    "    # print(read_wave_power(year, scenario))\n",
    "\n",
    "    # write_marine_generators(year, scenario)\n",
    "    # year_baseline = 2012\n",
    "    # add_marine_timeseries(year, year_baseline, scenario, time_step=0.5)\n",
    "    for scenario in ['Leading The Way', 'Falling Short', 'System Transformation', 'Consumer Transformation']:\n",
    "        for year in range(2021, 2050 + 1, 1):\n",
    "            for fes in [2021, 2022]:\n",
    "                if fes == 2021 and scenario == 'Falling Short':\n",
    "                    scenario = 'Steady Progression'\n",
    "                if fes == 2022 and scenario == 'Steady Progression':\n",
    "                    scenario = 'Falling Short'\n",
    "                print(scenario, year, fes)\n",
    "                # year = 2050\n",
    "                # scenario = 'Leading The Way'\n",
    "                # fes = 22\n",
    "                read_wave_power(year, scenario, fes)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
