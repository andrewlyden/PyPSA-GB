{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "import pandas as pd\n",
    "import distance_calculator as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_historical_demand_data():\n",
    "    \"\"\"reads the historical demand data from ESPENI database\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        ESPENI demand data, see dates below\n",
    "    \"\"\"\n",
    "\n",
    "    # this stuff can be used to get data from national grid csv files\n",
    "    # file1 = 'data/demand/DemandData_2005-2010.csv'\n",
    "    # file2 = 'data/demand/DemandData_2011-2016.csv'\n",
    "    # file3 = 'data/demand/DemandData_2017.csv'\n",
    "    # file4 = 'data/demand/DemandData_2018.csv'\n",
    "\n",
    "    # # reads csvs\n",
    "    # df1 = pd.read_csv(file1)\n",
    "    # df2 = pd.read_csv(file2)\n",
    "    # df3 = pd.read_csv(file3)\n",
    "    # df4 = pd.read_csv(file4)\n",
    "\n",
    "    # frames = [df2, df3, df4]\n",
    "    # df = df1.append(frames, ignore_index=True, sort=False)\n",
    "\n",
    "    # using espeni data set\n",
    "    file = '../data/demand/espeni.csv'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    dti = pd.date_range(\n",
    "        start='2008-11-05 22:00:00', end='2021-06-06 23:30:00', freq='0.5h')\n",
    "    df = df.set_index(dti)\n",
    "    df = df[['POWER_ESPENI_MW']]\n",
    "    # df = df.drop(columns=['SETTLEMENT_DATE', 'SETTLEMENT_PERIOD'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7504f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_future_profile_data():\n",
    "    \"\"\"reads the future demand profile data from Staffel et al - https://doi.org/10.1016/j.energy.2015.06.082\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        eload and DESSTINEE demand data, see dates below\n",
    "    \"\"\"\n",
    "\n",
    "    # this stuff can be used to get data from national grid csv files\n",
    "    # file1 = 'data/demand/DemandData_2005-2010.csv'\n",
    "    # file2 = 'data/demand/DemandData_2011-2016.csv'\n",
    "    # file3 = 'data/demand/DemandData_2017.csv'\n",
    "    # file4 = 'data/demand/DemandData_2018.csv'\n",
    "\n",
    "    # # reads csvs\n",
    "    # df1 = pd.read_csv(file1)\n",
    "    # df2 = pd.read_csv(file2)\n",
    "    # df3 = pd.read_csv(file3)\n",
    "    # df4 = pd.read_csv(file4)\n",
    "\n",
    "    # frames = [df2, df3, df4]\n",
    "    # df = df1.append(frames, ignore_index=True, sort=False)\n",
    "\n",
    "    # using espeni data set\n",
    "    file = '../data/demand/egy_7649_mmc1.xlsx'\n",
    "    df = pd.read_excel(file, sheet_name=None)\n",
    "    df_eload = df['ELOAD'].drop([0, 1, 2, 3, 4, 5])[['eLOAD Model (2050).2']].reset_index(drop=True) * 1000\n",
    "    df_eload.rename(columns={'eLOAD Model (2050).2': 'eLOAD'}, inplace=True)\n",
    "    dti = pd.date_range(\n",
    "        start='2050-01-01 00:00:00', end='2050-12-31 23:00:00', freq='h')\n",
    "    df_eload = df_eload.set_index(dti)\n",
    "    # resample to half hour frequency\n",
    "    df_eload['eLOAD'] = pd.to_numeric(df_eload['eLOAD'])\n",
    "    df_eload = df_eload.resample('0.5h').interpolate('polynomial', order=2)\n",
    "\n",
    "    # add end value\n",
    "    df_new_eload = pd.DataFrame(\n",
    "        data=df_eload.tail(1).values,\n",
    "        columns=df_eload.columns,\n",
    "        index = pd.date_range(start='2050-12-31 23:30:00', end='2050-12-31 23:30:00', freq='0.5h'))\n",
    "    # add to existing dataframe\n",
    "    df_eload = pd.concat([df_eload, df_new_eload], sort=False)\n",
    "\n",
    "    return df_eload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_loads(year, networkmodel='Reduced'):\n",
    "    \"\"\"writes the loads csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # LOADS CSV FILE\n",
    "    # first off the loads is just at a single bus\n",
    "    data = {'name': 'load', 'bus': 'bus'}\n",
    "    df = pd.DataFrame(data=data, index=[0])\n",
    "    df.to_csv('UC_data/loads.csv', index=False, header=True)\n",
    "\n",
    "    df_buses = pd.read_csv('LOPF_data/buses.csv')\n",
    "    df_buses = df_buses.drop(columns=['v_nom', 'carrier', 'x', 'y'])\n",
    "    df_buses['bus'] = df_buses['name']\n",
    "    df_buses = df_buses.set_index('name')\n",
    "    if year > 2020:\n",
    "        # delete the interconnectors if future years\n",
    "        try:\n",
    "            df_buses = df_buses.drop(['Belgium', 'France1', 'France2',\n",
    "                                    'Netherlands', 'Ireland', 'N. Ireland'])\n",
    "        except:\n",
    "            pass\n",
    "    # delete the IC loads\n",
    "    df_buses.to_csv('LOPF_data/loads.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4d48e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def write_loads_p_set(start, end, year, time_step, dataset, year_baseline=None, scenario=None, FES=None, scale_to_peak=False, networkmodel='Reduced'):\n",
    "    \"\"\"writes the loads power timeseries csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str\n",
    "        start of simulation period\n",
    "    end : str\n",
    "        end of simulation period\n",
    "    dataset : str\n",
    "        can be 'historical' or 'eload'\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # LOADS-P_SET CSV FILE\n",
    "    if dataset == 'historical':\n",
    "        df_hd = read_historical_demand_data()\n",
    "        df_hd.rename(columns={'POWER_ESPENI_MW': 'load'}, inplace=True)\n",
    "    elif dataset == 'eload':\n",
    "        df_hd = read_future_profile_data()\n",
    "        df_hd.rename(columns={'eLOAD': 'load'}, inplace=True)\n",
    "        # if modelled year is a leap year need to add in 29th feb (copy 28th Feb)\n",
    "        if year % 4 == 0:\n",
    "            df_hd.index = pd.to_datetime({\n",
    "                'year': 2040,\n",
    "                'month': df_hd.index.month,\n",
    "                'day': df_hd.index.day,\n",
    "                'hour': df_hd.index.hour,\n",
    "                'minute': df_hd.index.minute,\n",
    "                'second': df_hd.index.second})\n",
    "            df_leap_day = df_hd[pd.to_datetime('2040-02-28 00:00:00'):pd.to_datetime('2040-02-28 23:30:00')]\n",
    "            df_leap_day.index = pd.to_datetime({'year': df_leap_day.index.year,\n",
    "                                                'month': df_leap_day.index.month,\n",
    "                                                'day': 29,\n",
    "                                                'hour': df_leap_day.index.hour,\n",
    "                                                'minute': df_leap_day.index.minute,\n",
    "                                                'second': df_leap_day.index.second})\n",
    "            df_hd_appended = pd.concat([df_hd, df_leap_day])\n",
    "            df_hd = df_hd_appended.sort_index()\n",
    "\n",
    "    # need an index for the period to be simulated\n",
    "    if time_step == 0.5:\n",
    "        freq = '0.5h'\n",
    "    elif time_step == 1.0:\n",
    "        freq = 'h'\n",
    "    else:\n",
    "        raise Exception(\"Time step not recognised\")\n",
    "\n",
    "    dti = pd.date_range(\n",
    "        start=start,\n",
    "        end=end,\n",
    "        freq=freq)\n",
    "\n",
    "    df_distribution = pd.read_csv(\n",
    "        '../data/demand/Demand_Distribution.csv', index_col=0)\n",
    "    df_distribution = df_distribution.loc[:, ~df_distribution.columns.str.contains('^Unnamed')]\n",
    "    df_distribution.dropna(inplace=True)\n",
    "\n",
    "    if networkmodel == 'Reduced':\n",
    "        df_distribution = pd.read_csv(\n",
    "            '../data/demand/Demand_Distribution.csv', index_col=0)\n",
    "        df_distribution = df_distribution.loc[:, ~df_distribution.columns.str.contains('^Unnamed')]\n",
    "        df_distribution.dropna(inplace=True)\n",
    "    elif networkmodel == 'Zonal':\n",
    "        df_distribution = pd.read_csv(\n",
    "            '../data/demand/Demand_Distribution_Zonal.csv', index_col=0)\n",
    "\n",
    "    # for historical years using the 2020 FES distribution data\n",
    "    if year < 2020:\n",
    "        year_dist = 2020\n",
    "    elif year >= 2020:\n",
    "        year_dist = year\n",
    "\n",
    "    # normalise the data so distribution can be applied\n",
    "    data = df_distribution[[str(year_dist)]].T\n",
    "    normalised = data.values / data.values.sum()\n",
    "    norm = pd.DataFrame(data=normalised, columns=data.columns)\n",
    "    norm.index.name = 'name'\n",
    "\n",
    "    if year <= 2020 and dataset == 'historical':\n",
    "        # if using historical data then use this to\n",
    "        # distribute to different nodes\n",
    "        # locate from historical data load for simulated period\n",
    "        df = df_hd.loc[start:end]\n",
    "        df_loads_p_set_UC = df\n",
    "        # create empty dataframe to populate\n",
    "        df_loads_p_set_LOPF = pd.DataFrame(index=df.index)\n",
    "        for j in norm.columns:\n",
    "            # scale load for each node/bus\n",
    "            df_loads_p_set_LOPF[j] = df['load'] * norm[j].values\n",
    "\n",
    "        if time_step == 0.5:\n",
    "            df_loads_p_set_LOPF.index = dti\n",
    "        elif time_step == 1:\n",
    "            df_loads_p_set_LOPF = df_loads_p_set_LOPF.resample(freq).mean()\n",
    "        if time_step == 0.5:\n",
    "            df_loads_p_set_UC.index = dti\n",
    "        elif time_step == 1:\n",
    "            df_loads_p_set_UC = df_loads_p_set_UC.resample(freq).mean()\n",
    "\n",
    "    elif year > 2020:\n",
    "        # if future scenarios need to scale historical\n",
    "        # data using FES demand data\n",
    "        if scenario == 'Leading The Way':\n",
    "            scenario = 'Leading the Way'\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ED1', header=4, dtype=str)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ED1', header=4, dtype=str)\n",
    "        elif FES == None:\n",
    "            raise Exception(\"Please choose a FES year.\")\n",
    "            \n",
    "        df_FES_demand = df_FES.loc[df_FES['Data item'] == 'GBFES System Demand: Total']\n",
    "        df_FES_demand = df_FES_demand.loc[df_FES_demand['Scenario'] == scenario]\n",
    "        date = str(year) + '-01-01 00:00:00'\n",
    "        df_FES_demand.columns = df_FES_demand.columns.astype(str)\n",
    "        # future demand in GWh/yr\n",
    "        future_demand = float(df_FES_demand[date].values[0])\n",
    "\n",
    "        # scale historical demand using baseline year\n",
    "        year_start = str(year_baseline) + '-01-01 00:00:00'\n",
    "        year_end = str(year_baseline) + '-12-31 23:30:00'\n",
    "        # using the baseline year as basis for demand distribution\n",
    "\n",
    "        if dataset == 'historical':\n",
    "            # load timeseries in baseline year\n",
    "            df_year = df_hd.loc[year_start:year_end]\n",
    "        elif dataset == 'eload':\n",
    "            df_year = df_hd\n",
    "\n",
    "        # summed load in baseline year\n",
    "        # factor of two because historical demand is read in half hourly, so need to convert to GWh/yr\n",
    "        historical_demand = df_year.sum().values[0] * 0.5 / 1000\n",
    "        scale_factor = float(future_demand) / float(historical_demand)\n",
    "\n",
    "        # load for simulation dates in baseline year\n",
    "        if dataset == 'historical':\n",
    "            yr = str(year_baseline)[-2:]\n",
    "            start_yr = start[:2] + yr + start[4:]\n",
    "            end_yr = end[:2] + yr + end[4:]\n",
    "            df_sim = df_year[start_yr:end_yr]\n",
    "        elif dataset == 'eload':\n",
    "            if year % 4 == 0:\n",
    "                yr = '40'\n",
    "            else:\n",
    "                yr = '50'\n",
    "            start_yr = start[:2] + yr + start[4:]\n",
    "            end_yr = end[:2] + yr + end[4:]\n",
    "            df_sim = df_year[start_yr:end_yr]\n",
    "        scaled_load = scale_factor * df_sim\n",
    "\n",
    "        # check if baseline year is a leap year and simulated year is not and remove 29th Feb\n",
    "        if year_baseline % 4 == 0:\n",
    "            # and the year modelled is also not a leap year\n",
    "            if year % 4 != 0:\n",
    "                # remove 29th Feb\n",
    "                scaled_load = scaled_load[~((scaled_load.index.month == 2) & (scaled_load.index.day == 29))]\n",
    "\n",
    "        if time_step == 0.5:\n",
    "            scaled_load.index = dti\n",
    "        elif time_step == 1:\n",
    "            scaled_load = scaled_load.resample(freq).mean()\n",
    "            # for some reason need to get rid of this date again?\n",
    "            # probably due to resampling step...\n",
    "            if year_baseline % 4 == 0:\n",
    "                # and the year modelled is also not a leap year\n",
    "                if year % 4 != 0:\n",
    "                    # remove 29th Feb\n",
    "                    scaled_load = scaled_load[~((scaled_load.index.month == 2) & (scaled_load.index.day == 29))]\n",
    "            scaled_load.index = dti\n",
    "\n",
    "        # can use this for UC\n",
    "        df_loads_p_set_UC = scaled_load.copy()\n",
    "\n",
    "        # need to distribute for the LOPF\n",
    "        df_loads_p_set_LOPF = pd.DataFrame(index=dti)\n",
    "        for j in norm.columns:\n",
    "            df_loads_p_set_LOPF[j] = scaled_load * norm[j].values\n",
    "    \n",
    "    if FES == 2022 and scale_to_peak == True:\n",
    "        # if FES is 22 then going to scale again using the peak demand from regional breakdown\n",
    "        df_year_LOPF = pd.DataFrame()\n",
    "        for j in norm.columns:\n",
    "            df_year_LOPF[j] = df_year * scale_factor * norm[j].values\n",
    "        peak_bus_regional = read_regional_breakdown_load(scenario, year, networkmodel)\n",
    "        for bus in df_year_LOPF.columns:\n",
    "            scaling_factor = peak_bus_regional[bus] / (df_year_LOPF[bus].max())\n",
    "            df_loads_p_set_LOPF[bus] *= scaling_factor\n",
    "\n",
    "    df_loads_p_set_UC.index.name = 'name'\n",
    "    df_loads_p_set_LOPF.index.name = 'name'\n",
    "\n",
    "    # if time_step == 0.5:\n",
    "\n",
    "    #     appendix = df_loads_p_set_LOPF.iloc[-1:]\n",
    "    #     new_index = df_loads_p_set_LOPF.index[-1] + pd.Timedelta(minutes=30)\n",
    "    #     appendix.rename(index={appendix.index[0]: new_index}, inplace=True)\n",
    "    #     df_loads_p_set_LOPF = df_loads_p_set_LOPF.append(appendix)\n",
    "\n",
    "    #     appendix = df_loads_p_set_UC.iloc[-1:]\n",
    "    #     new_index = df_loads_p_set_LOPF.index[-1] + pd.Timedelta(minutes=30)\n",
    "    #     appendix.rename(index={appendix.index[0]: new_index}, inplace=True)\n",
    "    #     df_loads_p_set_UC = df_loads_p_set_UC.append(appendix)\n",
    "\n",
    "    df_loads_p_set_LOPF.to_csv('LOPF_data/loads-p_set.csv', header=True)\n",
    "    df_loads_p_set_UC.to_csv('UC_data/loads-p_set.csv', header=True)\n",
    "\n",
    "    return df_loads_p_set_LOPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77e792",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_regional_breakdown_load(scenario, year, networkmodel):\n",
    "\n",
    "    if scenario == 'Leading the Way':\n",
    "        df_regional = pd.read_excel('../data/FES2022/FES22_regional_peak_load_leading_the_way.xlsx', sheet_name=str(year), header=5)\n",
    "    elif scenario == 'Consumer Transformation':\n",
    "        df_regional = pd.read_excel('../data/FES2022/FES22_regional_peak_load_consumer_transformation.xlsx', sheet_name=str(year), header=5)\n",
    "    elif scenario == 'System Transformation':\n",
    "        df_regional = pd.read_excel('../data/FES2022/FES22_regional_peak_load_system_transformation.xlsx', sheet_name=str(year), header=5)\n",
    "    elif scenario == 'Falling Short':\n",
    "        df_regional = pd.read_excel('../data/FES2022/FES22_regional_peak_load_falling_short.xlsx', sheet_name=str(year), header=5)\n",
    "    # remove first row\n",
    "    df_regional = df_regional.iloc[1: , :].set_index('Name')\n",
    "    # delete all duplicates in index (removes electrolysis stuff which is zero for winter peak anyway)\n",
    "    df_regional = df_regional[~df_regional.index.duplicated(keep=False)]\n",
    "    # print(df_regional['P(Gross'])\n",
    "    df_regional = df_regional[['P(Gross)']]\n",
    "    df_regional = df_regional.loc[~(df_regional==0).all(axis=1)]\n",
    "\n",
    "    df_gsp_data = pd.read_csv('../data/FES2022/GSP_data.csv', encoding='cp1252', index_col=3)\n",
    "    df_gsp_data = df_gsp_data[['Latitude', 'Longitude']]\n",
    "    df_gsp_data.rename(columns={'Latitude': 'y', 'Longitude': 'x'}, inplace=True)\n",
    "\n",
    "    if networkmodel == 'Reduced':\n",
    "        from distance_calculator import map_to_bus as map_to\n",
    "    elif networkmodel == 'Zonal':\n",
    "        from allocate_to_zone import map_to_zone as map_to\n",
    "\n",
    "    df_gsp_data['Bus'] = map_to(df_gsp_data)\n",
    "    # now\n",
    "    GSP_to_bus = []\n",
    "    for GSP in df_regional.index:\n",
    "        GSP_to_bus.append(df_gsp_data['Bus'][GSP])\n",
    "\n",
    "    df_regional['Bus'] = GSP_to_bus\n",
    "    # list of buses\n",
    "    peak_bus = {}\n",
    "    for bus in df_regional['Bus'].unique():\n",
    "        peak_bus[bus] = df_regional.loc[df_regional['Bus'] == bus]['P(Gross)'].sum()\n",
    "    \n",
    "    return peak_bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcd309",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def distribution_zonal_loads():\n",
    "    scenario = 'Leading the Way'\n",
    "    data = {}\n",
    "    for year in range(2021, 2051, 1):\n",
    "        dic = read_regional_breakdown_load(scenario, year, networkmodel='Zonal')\n",
    "        data[year] = pd.Series(data=dic.values(), index=dic.keys(), name=year)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('../data/demand/Demand_Distribution_Zonal.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    read_future_profile_data()\n",
    "    # read_historical_demand_data()\n",
    "    # read_regional_breakdown_load()\n",
    "    # distribution_zonal_loads()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
