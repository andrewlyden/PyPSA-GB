{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bda1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import renewables\n",
    "from utils.cleaning import unify_index\n",
    "from utils.cleaning import remove_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_power_stations_data(year):\n",
    "    \"\"\"reads the power station data from prepared csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        power station data\n",
    "    \"\"\"\n",
    "    # short term fix to run data_reader_writer.py\n",
    "    if year > 2020:\n",
    "        year = 2020\n",
    "\n",
    "    # want to read in the conventional power generators from\n",
    "    # the data prepared from DUKES\n",
    "    file = '../data/power stations/power_stations_locations_' + str(year) + '.csv'\n",
    "    # read the csv\n",
    "    df = pd.read_csv(file, encoding='unicode_escape')\n",
    "    # fix the formatting to have each element be a list of\n",
    "    # latitude [0] and longitude [1]\n",
    "    df['Geolocation'] = df['Geolocation'].str.replace(',', '')\n",
    "    df['Geolocation'] = df['Geolocation'].str.split()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_generator_data_by_fuel():\n",
    "    \"\"\"reads the generator data by fuel from prepared csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        generator data by fuel\n",
    "    \"\"\"\n",
    "\n",
    "    file = '../data/generator_data_by_fuel.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.set_index('fuel')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d5827",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def write_generators(time_step, year, networkmodel=True):\n",
    "    if networkmodel:\n",
    "        from distance_calculator import map_to_bus as map_to\n",
    "    else:\n",
    "        from allocate_to_zone import map_to_zone as map_to\n",
    "    \"\"\"writes the generators csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int/str\n",
    "        year of simulation\n",
    "    time_step : float\n",
    "        defined as fraction of an hour, e.g., 0.5 is half hour\n",
    "        currently set up as only hour or half hour\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    # GENERATOR CSV FILE\n",
    "    # read in conventional power plants\n",
    "    df_pp = read_power_stations_data(year)\n",
    "\n",
    "    df_pp = df_pp.drop(\n",
    "        columns=['Company Name', 'Year of commission or year generation began',\n",
    "                 'Location'])\n",
    "    df_pp = df_pp.rename(columns={'Installed Capacity (MW)': 'p_nom', 'Fuel': 'carrier',\n",
    "                                  'Technology': 'type', 'Station Name': 'name'})\n",
    "\n",
    "    # only do this if there is comma in power\n",
    "    # pass this step otherwise\n",
    "    try:\n",
    "        df_pp['p_nom'] = df_pp['p_nom'].str.replace(',', '')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_pp_UC = df_pp.drop(\n",
    "        columns=['x', 'y', 'Geolocation'])\n",
    "    df_pp_UC['bus'] = 'bus'\n",
    "\n",
    "    df_pp_LOPF = df_pp.drop(\n",
    "        columns=['x', 'y', 'Geolocation'])\n",
    "    df_pp_LOPF['bus'] = map_to(df_pp)\n",
    "\n",
    "    # corrections factors for RES generators: Onshore, Offshore, PV\n",
    "    df_correction = pd.read_csv('../data/renewables/atlite/RES_correction_factors.csv', index_col=0)\n",
    "\n",
    "    # read in the renewable generators\n",
    "    df_res = renewables.REPD_date_corrected(year)\n",
    "    # start with the offshore wind farms\n",
    "    df_res_offshore = df_res.loc[df_res['Technology Type'] == 'Wind Offshore'].reset_index(drop=True)\n",
    "    df_res_offshore = df_res_offshore.drop(\n",
    "        columns=['CHP Enabled', 'No. of Turbines', 'Development Status',\n",
    "                 'X-coordinate', 'Y-coordinate', 'Operational',\n",
    "                 'Height of Turbines (m)', 'Mounting Type for Solar',\n",
    "                 'Turbine Capacity (MW)'])\n",
    "    df_res_offshore = df_res_offshore.rename(columns={'Installed Capacity (MWelec)': 'p_nom',\n",
    "                                                      'Technology Type': 'carrier',\n",
    "                                                      'Site Name': 'name',\n",
    "                                                      'lon': 'x',\n",
    "                                                      'lat': 'y'})\n",
    "    df_res_offshore['type'] = 'Wind Offshore'\n",
    "\n",
    "    # scale the wind offshore to real data, see correction factors in Atlite\n",
    "    if year <= 2020:\n",
    "        df_res_offshore.loc[:, 'p_nom'] *= df_correction.loc['Wind_Offshore', str(year)]\n",
    "\n",
    "    # add in future sites for future scenarios\n",
    "    if year > 2020:\n",
    "        path = '../data/renewables/future_offshore_sites/'\n",
    "        file1 = 'offshore_pipeline.csv'\n",
    "        df_pipeline = pd.read_csv(path + file1, encoding='unicode_escape')\n",
    "        df_pipeline.drop(columns=['Record Last Updated (dd/mm/yyyy)', 'Operator (or Applicant)',\n",
    "                                  'Under Construction', 'Technology Type',\n",
    "                                  'Planning Permission Expired', 'Operational',\n",
    "                                  'Heat Network Ref', 'Planning Authority',\n",
    "                                  'Planning Application Submitted', 'Region',\n",
    "                                  'Country', 'County', 'Expected Operational',\n",
    "                                  'Turbine Capacity (MW)', 'No. of Turbines',\n",
    "                                  'Development Status', 'Development Status (short)'], inplace=True)\n",
    "        df_pipeline.dropna(axis='columns', inplace=True)\n",
    "        df_pipeline['carrier'] = 'Wind Offshore'\n",
    "\n",
    "        # create two lists of conversions from OSGB to lat/lon\n",
    "        lon = []\n",
    "        lat = []\n",
    "        for i in range(len(df_pipeline.index)):\n",
    "            x = df_pipeline['X-coordinate'][i]\n",
    "            y = df_pipeline['Y-coordinate'][i]\n",
    "            coord = osgb.grid_to_ll(x, y)\n",
    "            lat.append(coord[0])\n",
    "            lon.append(coord[1])\n",
    "        df_pipeline['lon'] = lon\n",
    "        df_pipeline['lat'] = lat\n",
    "        df_pipeline['type'] = df_pipeline['carrier']\n",
    "        df_pipeline.rename(columns={'Installed Capacity (MWelec)': 'p_nom',\n",
    "                                    'Site Name': 'name',\n",
    "                                    'lon': 'x',\n",
    "                                    'lat': 'y'}, inplace=True)\n",
    "        df_pipeline.drop(columns=['X-coordinate', 'Y-coordinate'], inplace=True)\n",
    "\n",
    "        file2 = 'Sectoral Marine Plan 2020 - Fixed.csv'\n",
    "        df_future_FBOW = pd.read_csv(path + file2, encoding='unicode_escape')\n",
    "        df_future_FBOW['carrier'] = 'Wind Offshore'\n",
    "        df_future_FBOW['type'] = 'Wind Offshore'\n",
    "        df_future_FBOW.drop(columns=['area (km2)'], inplace=True)\n",
    "        df_future_FBOW.rename(columns={'max capacity (GW)': 'p_nom',\n",
    "                                  'lon': 'x',\n",
    "                                  'lat': 'y'}, inplace=True)\n",
    "        \n",
    "        file3 = 'Sectoral Marine Plan 2020 - Floating.csv'\n",
    "        df_future_FOW = pd.read_csv(path + file3, encoding='unicode_escape')\n",
    "        df_future_FOW['carrier'] = 'Wind Offshore'\n",
    "        df_future_FOW['type'] = 'Floating Wind'\n",
    "        df_future_FOW.drop(columns=['area (km2)'], inplace=True)\n",
    "        df_future_FOW.rename(columns={'max capacity (GW)': 'p_nom',\n",
    "                                  'lon': 'x',\n",
    "                                  'lat': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a95f",
   "metadata": {},
   "source": [
    "        df_future = df_future.append([df_future_FBOW, df_future_FOW], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # convert from GW to MW\n",
    "        df_future_FBOW.loc[:, 'p_nom'] *= 1000\n",
    "        df_future_FOW.loc[:, 'p_nom'] *= 1000\n",
    "\n",
    "        df_res_offshore = pd.concat([df_res_offshore, df_pipeline, df_future_FBOW, df_future_FOW], ignore_index=True)     \n",
    "      \n",
    "    df_res_offshore_UC = df_res_offshore.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_offshore_UC['bus'] = 'bus'\n",
    "\n",
    "    df_res_offshore_LOPF = df_res_offshore.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_offshore_LOPF['bus'] = map_to(df_res_offshore)\n",
    "    \n",
    "    \n",
    "    # join to previous df of thermal power plants\n",
    "    df_UC = pd.concat([df_pp_UC, df_res_offshore_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_pp_LOPF, df_res_offshore_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # check names are unique for UC\n",
    "    duplicateDFRow = df_UC[df_UC.duplicated(['name'], keep='first')]\n",
    "    for i in range(len(duplicateDFRow.index.values)):\n",
    "        # print(df_UC['name'][duplicateDFRow.index.values[i]])\n",
    "        df_UC.at[duplicateDFRow.index.values[i], 'name'] = (\n",
    "            df_UC['name'][duplicateDFRow.index.values[i]] + '.1')\n",
    "        # print(df_UC['name'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "    # check names are unique for LOPF\n",
    "    duplicateDFRow = df_pp_LOPF[df_pp_LOPF.duplicated(['name'], keep='first')]\n",
    "    for i in range(len(duplicateDFRow.index.values)):\n",
    "        # print(df_pp_LOPF['name'][duplicateDFRow.index.values[i]])\n",
    "        df_pp_LOPF.at[duplicateDFRow.index.values[i], 'name'] = (\n",
    "            df_pp_LOPF['name'][duplicateDFRow.index.values[i]] + '.1')\n",
    "        # print(df_pp_LOPF['name'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "    # then the onshore wind farms\n",
    "    df_res_onshore = df_res.loc[df_res['Technology Type'] == 'Wind Onshore'].reset_index(drop=True)\n",
    "    df_res_onshore = df_res_onshore.drop(\n",
    "        columns=['CHP Enabled', 'No. of Turbines', 'Development Status',\n",
    "                 'X-coordinate', 'Y-coordinate', 'Operational',\n",
    "                 'Height of Turbines (m)', 'Mounting Type for Solar',\n",
    "                 'Turbine Capacity (MW)'])\n",
    "    df_res_onshore = df_res_onshore.rename(columns={'Installed Capacity (MWelec)': 'p_nom',\n",
    "                                                    'Technology Type': 'carrier',\n",
    "                                                    'Site Name': 'name',\n",
    "                                                    'lon': 'x',\n",
    "                                                    'lat': 'y'})\n",
    "    df_res_onshore['type'] = 'Wind Onshore'\n",
    "\n",
    "    # scale the wind onshore to real data, see correction factors in Atlite\n",
    "    if year <= 2020:\n",
    "        df_res_onshore.loc[:, 'p_nom'] *= df_correction.loc['Wind_Onshore', str(year)]\n",
    "\n",
    "    df_res_onshore_UC = df_res_onshore.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_onshore_UC['bus'] = 'bus'\n",
    "\n",
    "    df_res_onshore_LOPF = df_res_onshore.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_onshore_LOPF['bus'] = map_to(df_res_onshore)\n",
    "\n",
    "    # join to previous df of thermal power plants\n",
    "    df_UC = pd.concat([df_UC, df_res_onshore_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_LOPF, df_res_onshore_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # then the PV farms\n",
    "    df_res_PV = df_res.loc[df_res['Technology Type'] == 'Solar Photovoltaics'].reset_index(drop=True)\n",
    "    df_res_PV = df_res_PV.drop(\n",
    "        columns=['CHP Enabled', 'No. of Turbines', 'Development Status',\n",
    "                 'X-coordinate', 'Y-coordinate', 'Operational',\n",
    "                 'Height of Turbines (m)', 'Mounting Type for Solar',\n",
    "                 'Turbine Capacity (MW)'])\n",
    "    df_res_PV = df_res_PV.rename(columns={'Installed Capacity (MWelec)': 'p_nom',\n",
    "                                          'Technology Type': 'carrier',\n",
    "                                          'Site Name': 'name',\n",
    "                                          'lon': 'x',\n",
    "                                          'lat': 'y'})\n",
    "    df_res_PV['type'] = 'Solar Photovoltaics'\n",
    "\n",
    "    # scale the PV to real data, see correction factors in Atlite\n",
    "    if year <= 2020:\n",
    "        df_res_PV.loc[:, 'p_nom'] *= df_correction.loc['PV', str(year)]\n",
    "\n",
    "    df_res_PV_UC = df_res_PV.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_PV_UC['bus'] = 'bus'\n",
    "\n",
    "    df_res_PV_LOPF = df_res_PV.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_res_PV_LOPF['bus'] = map_to(df_res_PV)\n",
    "\n",
    "    # join to previous df of thermal power plants and offshore wind\n",
    "    df_UC = pd.concat([df_UC, df_res_PV_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_LOPF, df_res_PV_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # add hydro data\n",
    "    df_hydro = renewables.read_hydro(year)\n",
    "    # drop pumped storage as this is going to be a storage unit\n",
    "    df_hydro = df_hydro[~df_hydro.type.str.contains(\"Pumped Storage Hydroelectricity\")]\n",
    "    df_hydro = df_hydro.rename(columns={'lon': 'x',\n",
    "                                        'lat': 'y'})\n",
    "    df_hydro['x'] = df_hydro['x'].astype(float)\n",
    "    df_hydro['y'] = df_hydro['y'].astype(float)\n",
    "\n",
    "    # UC bus is same\n",
    "    df_hydro_UC = df_hydro.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_hydro_UC['bus'] = 'bus'\n",
    "\n",
    "    df_hydro_LOPF = df_hydro.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_hydro_LOPF['bus'] = map_to(df_hydro)\n",
    "\n",
    "    # join to previous df of thermal power plants and offshore wind\n",
    "    df_UC = pd.concat([df_UC, df_hydro_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_LOPF, df_hydro_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # add non dispatchable generators \"RES\"\n",
    "    df_NDC = renewables.read_non_dispatchable_continuous(year)\n",
    "\n",
    "    df_NDC = df_NDC.rename(columns={'lon': 'x',\n",
    "                                    'lat': 'y'})\n",
    "\n",
    "    # UC bus is same\n",
    "    df_NDC_UC = df_NDC.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_NDC_UC['bus'] = 'bus'\n",
    "\n",
    "    df_NDC_LOPF = df_NDC.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_NDC_LOPF['bus'] = map_to(df_NDC)\n",
    "\n",
    "    # join to previous df of thermal power plants and offshore wind\n",
    "    df_UC = pd.concat([df_UC, df_NDC_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_LOPF, df_NDC_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # add biomass boilers\n",
    "    df_bio = renewables.read_biomass(year)\n",
    "    df_bio = df_bio.rename(columns={'lon': 'x',\n",
    "                                    'lat': 'y'})\n",
    "\n",
    "    # UC bus is same\n",
    "    df_bio_UC = df_bio.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_bio_UC['bus'] = 'bus'\n",
    "\n",
    "    df_bio_LOPF = df_bio.drop(\n",
    "        columns=['x', 'y'])\n",
    "    df_bio_LOPF['bus'] = map_to(df_bio)\n",
    "\n",
    "    # join to previous df of thermal power plants and offshore wind\n",
    "    df_UC = pd.concat([df_UC, df_bio_UC], ignore_index=True, sort=False)\n",
    "    df_LOPF = pd.concat([df_LOPF, df_bio_LOPF], ignore_index=True, sort=False)\n",
    "\n",
    "    # run additional data for both UC and LOPF\n",
    "    df_UC = generator_additional_data(df_UC, time_step)\n",
    "    df_LOPF = generator_additional_data(df_LOPF, time_step)\n",
    "    # remove the unit committent constraints\n",
    "    df_LOPF = df_LOPF.drop(\n",
    "        columns=['committable', 'min_up_time', 'min_down_time',\n",
    "                 'p_min_pu', 'up_time_before', 'start_up_cost'])\n",
    "\n",
    "    # remove a-cirumflex characters\n",
    "    # cols = df_LOPF.select_dtypes(include=[np.object]).columns\n",
    "    # df_LOPF[cols] = df_LOPF[cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))\n",
    "    df_LOPF['name'] = df_LOPF['name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df_LOPF['name'] = df_LOPF['name'].astype(str).str.replace(u'\\xa0', '')\n",
    "    df_LOPF['name'] = df_LOPF['name'].astype(str).str.replace('ì', 'i')\n",
    "    df_LOPF['name'] = df_LOPF['name'].str.strip()\n",
    "\n",
    "    # df_UC[cols] = df_UC[cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))\n",
    "    df_UC['name'] = df_UC['name'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df_UC['name'] = df_UC['name'].astype(str).str.replace(u'\\xa0', '')\n",
    "    df_UC['name'] = df_UC['name'].astype(str).str.replace('ì', 'i')\n",
    "    df_UC['name'] = df_UC['name'].str.strip()\n",
    "\n",
    "    # check names are unique for UC\n",
    "    duplicateDFRow = df_UC[df_UC.duplicated(['name'], keep='first')]\n",
    "    for i in range(len(duplicateDFRow.index.values)):\n",
    "        # print(df_UC['name'][duplicateDFRow.index.values[i]])\n",
    "        df_UC.at[duplicateDFRow.index.values[i], 'name'] = (\n",
    "            df_UC['name'][duplicateDFRow.index.values[i]] + '.1')\n",
    "        # print(df_UC['name'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "    # check names are unique for LOPF\n",
    "    duplicateDFRow = df_LOPF[df_LOPF.duplicated(['name'], keep='first')]\n",
    "    for i in range(len(duplicateDFRow.index.values)):\n",
    "        # print(df_LOPF['name'][duplicateDFRow.index.values[i]])\n",
    "        df_LOPF.at[duplicateDFRow.index.values[i], 'name'] = (\n",
    "            df_LOPF['name'][duplicateDFRow.index.values[i]] + '.1')\n",
    "        # print(df_LOPF['name'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "    # save the dataframes to csv\n",
    "    df_UC.to_csv('UC_data/generators.csv', index=False, header=True)\n",
    "    df_LOPF.to_csv('LOPF_data/generators.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7df0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_additional_data(df, time_step):\n",
    "    \"\"\"adds data to the generators csv file\n",
    "\n",
    "    add marginal costs, min up/down time, ramp up/down rates\n",
    "    to the generators csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        dataframe containing the generator data to be added to\n",
    "    time_step : float\n",
    "        defined as fraction of an hour, e.g., 0.5 is half hour\n",
    "        currently set up as only hour or half hour\n",
    "    Returns\n",
    "    dataframe\n",
    "        data with all required generator for PyPSA UC or LOPF\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    # add marginal costs, min up/down time, ramp up/down rates\n",
    "    df_data = read_generator_data_by_fuel()\n",
    "\n",
    "    conditions = [\n",
    "        (df['carrier'] == 'Coal'),\n",
    "        (df['carrier'] == 'Oil'),\n",
    "        (df['carrier'] == 'Natural Gas') & (df['type'] == 'CCGT'),\n",
    "        (df['carrier'] == 'Natural Gas') & (df['type'] == 'OCGT'),\n",
    "        (df['carrier'] == 'Natural Gas') & (df['type'] == 'Sour gas'),\n",
    "        (df['carrier'] == 'Nuclear'),\n",
    "        (df['carrier'] == 'Wind Offshore'),\n",
    "        (df['carrier'] == 'Large Hydro'),\n",
    "        (df['carrier'] == 'Small Hydro'),\n",
    "        (df['carrier'] == 'Anaerobic Digestion'),\n",
    "        (df['carrier'] == 'EfW Incineration'),\n",
    "        (df['carrier'] == 'Landfill Gas'),\n",
    "        (df['carrier'] == 'Sewage Sludge Digestion'),\n",
    "        (df['carrier'] == 'Shoreline Wave'),\n",
    "        (df['carrier'] == 'Tidal Barrage and Tidal Stream'),\n",
    "        (df['carrier'] == 'Biomass (dedicated)'),\n",
    "        (df['carrier'] == 'Biomass (co-firing)'),\n",
    "        (df['carrier'] == 'Wind Onshore'),\n",
    "        (df['carrier'] == 'Solar Photovoltaics'),\n",
    "        (df['carrier'] == 'CCS Gas'),\n",
    "        (df['carrier'] == 'CCS Biomass'),\n",
    "        (df['carrier'] == 'Hydrogen')]\n",
    "\n",
    "    marg_cos = [df_data['marginal_costs']['Coal'],\n",
    "                df_data['marginal_costs']['Oil'],\n",
    "                df_data['marginal_costs']['CCGT'],\n",
    "                df_data['marginal_costs']['OCGT'],\n",
    "                df_data['marginal_costs']['Sour gas'],\n",
    "                df_data['marginal_costs']['Nuclear'],\n",
    "                df_data['marginal_costs']['Wind Offshore'],\n",
    "                df_data['marginal_costs']['Large Hydro'],\n",
    "                df_data['marginal_costs']['Small Hydro'],\n",
    "                df_data['marginal_costs']['Anaerobic Digestion'],\n",
    "                df_data['marginal_costs']['EfW Incineration'],\n",
    "                df_data['marginal_costs']['Landfill Gas'],\n",
    "                df_data['marginal_costs']['Sewage Sludge Digestion'],\n",
    "                df_data['marginal_costs']['Shoreline Wave'],\n",
    "                df_data['marginal_costs']['Tidal Barrage and Tidal Stream'],\n",
    "                df_data['marginal_costs']['Biomass (dedicated)'],\n",
    "                df_data['marginal_costs']['Biomass (co-firing)'],\n",
    "                df_data['marginal_costs']['Wind Onshore'],\n",
    "                df_data['marginal_costs']['Solar Photovoltaics'],\n",
    "                df_data['marginal_costs']['CCS Gas'],\n",
    "                df_data['marginal_costs']['CCS Biomass'],\n",
    "                df_data['marginal_costs']['Hydrogen']]\n",
    "\n",
    "    min_up_time_ = [df_data['min_up_time']['Coal'],\n",
    "                    df_data['min_up_time']['Oil'],\n",
    "                    df_data['min_up_time']['CCGT'],\n",
    "                    df_data['min_up_time']['OCGT'],\n",
    "                    df_data['min_up_time']['Sour gas'],\n",
    "                    df_data['min_up_time']['Nuclear'],\n",
    "                    df_data['min_up_time']['Wind Offshore'],\n",
    "                    df_data['min_up_time']['Large Hydro'],\n",
    "                    df_data['min_up_time']['Small Hydro'],\n",
    "                    df_data['min_up_time']['Anaerobic Digestion'],\n",
    "                    df_data['min_up_time']['EfW Incineration'],\n",
    "                    df_data['min_up_time']['Landfill Gas'],\n",
    "                    df_data['min_up_time']['Sewage Sludge Digestion'],\n",
    "                    df_data['min_up_time']['Shoreline Wave'],\n",
    "                    df_data['min_up_time']['Tidal Barrage and Tidal Stream'],\n",
    "                    df_data['min_up_time']['Biomass (dedicated)'],\n",
    "                    df_data['min_up_time']['Biomass (co-firing)'],\n",
    "                    df_data['min_up_time']['Wind Onshore'],\n",
    "                    df_data['min_up_time']['Solar Photovoltaics'],\n",
    "                    df_data['min_up_time']['CCS Gas'],\n",
    "                    df_data['min_up_time']['CCS Biomass'],\n",
    "                    df_data['min_up_time']['Hydrogen']]\n",
    "\n",
    "    min_down_time_ = [df_data['min_down_time']['Coal'],\n",
    "                      df_data['min_down_time']['Oil'],\n",
    "                      df_data['min_down_time']['CCGT'],\n",
    "                      df_data['min_down_time']['OCGT'],\n",
    "                      df_data['min_down_time']['Sour gas'],\n",
    "                      df_data['min_down_time']['Nuclear'],\n",
    "                      df_data['min_down_time']['Wind Offshore'],\n",
    "                      df_data['min_down_time']['Large Hydro'],\n",
    "                      df_data['min_down_time']['Small Hydro'],\n",
    "                      df_data['min_down_time']['Anaerobic Digestion'],\n",
    "                      df_data['min_down_time']['EfW Incineration'],\n",
    "                      df_data['min_down_time']['Landfill Gas'],\n",
    "                      df_data['min_down_time']['Sewage Sludge Digestion'],\n",
    "                      df_data['min_down_time']['Shoreline Wave'],\n",
    "                      df_data['min_down_time']['Tidal Barrage and Tidal Stream'],\n",
    "                      df_data['min_down_time']['Biomass (dedicated)'],\n",
    "                      df_data['min_down_time']['Biomass (co-firing)'],\n",
    "                      df_data['min_down_time']['Wind Onshore'],\n",
    "                      df_data['min_down_time']['Solar Photovoltaics'],\n",
    "                      df_data['min_down_time']['CCS Gas'],\n",
    "                      df_data['min_down_time']['CCS Biomass'],\n",
    "                      df_data['min_down_time']['Hydrogen']]\n",
    "\n",
    "    ramp_limit_up_ = [df_data['ramp_limit_up']['Coal'],\n",
    "                      df_data['ramp_limit_up']['Oil'],\n",
    "                      df_data['ramp_limit_up']['CCGT'],\n",
    "                      df_data['ramp_limit_up']['OCGT'],\n",
    "                      df_data['ramp_limit_up']['Sour gas'],\n",
    "                      df_data['ramp_limit_up']['Nuclear'],\n",
    "                      df_data['ramp_limit_up']['Wind Offshore'],\n",
    "                      df_data['ramp_limit_up']['Large Hydro'],\n",
    "                      df_data['ramp_limit_up']['Small Hydro'],\n",
    "                      df_data['ramp_limit_up']['Anaerobic Digestion'],\n",
    "                      df_data['ramp_limit_up']['EfW Incineration'],\n",
    "                      df_data['ramp_limit_up']['Landfill Gas'],\n",
    "                      df_data['ramp_limit_up']['Sewage Sludge Digestion'],\n",
    "                      df_data['ramp_limit_up']['Shoreline Wave'],\n",
    "                      df_data['ramp_limit_up']['Tidal Barrage and Tidal Stream'],\n",
    "                      df_data['ramp_limit_up']['Biomass (dedicated)'],\n",
    "                      df_data['ramp_limit_up']['Biomass (co-firing)'],\n",
    "                      df_data['ramp_limit_up']['Wind Onshore'],\n",
    "                      df_data['ramp_limit_up']['Solar Photovoltaics'],\n",
    "                      df_data['ramp_limit_up']['CCS Gas'],\n",
    "                      df_data['ramp_limit_up']['CCS Biomass'],\n",
    "                      df_data['ramp_limit_up']['Hydrogen']]\n",
    "\n",
    "    ramp_limit_down_ = [df_data['ramp_limit_down']['Coal'],\n",
    "                        df_data['ramp_limit_down']['Oil'],\n",
    "                        df_data['ramp_limit_down']['CCGT'],\n",
    "                        df_data['ramp_limit_down']['OCGT'],\n",
    "                        df_data['ramp_limit_down']['Sour gas'],\n",
    "                        df_data['ramp_limit_down']['Nuclear'],\n",
    "                        df_data['ramp_limit_down']['Wind Offshore'],\n",
    "                        df_data['ramp_limit_down']['Large Hydro'],\n",
    "                        df_data['ramp_limit_down']['Small Hydro'],\n",
    "                        df_data['ramp_limit_down']['Anaerobic Digestion'],\n",
    "                        df_data['ramp_limit_down']['EfW Incineration'],\n",
    "                        df_data['ramp_limit_down']['Landfill Gas'],\n",
    "                        df_data['ramp_limit_down']['Sewage Sludge Digestion'],\n",
    "                        df_data['ramp_limit_down']['Shoreline Wave'],\n",
    "                        df_data['ramp_limit_down']['Tidal Barrage and Tidal Stream'],\n",
    "                        df_data['ramp_limit_down']['Biomass (dedicated)'],\n",
    "                        df_data['ramp_limit_down']['Biomass (co-firing)'],\n",
    "                        df_data['ramp_limit_down']['Wind Onshore'],\n",
    "                        df_data['ramp_limit_down']['Solar Photovoltaics'],\n",
    "                        df_data['ramp_limit_down']['CCS Gas'],\n",
    "                        df_data['ramp_limit_down']['CCS Biomass'],\n",
    "                        df_data['ramp_limit_down']['Hydrogen']]\n",
    "\n",
    "    committable_ = [df_data['committable']['Coal'],\n",
    "                    df_data['committable']['Oil'],\n",
    "                    df_data['committable']['CCGT'],\n",
    "                    df_data['committable']['OCGT'],\n",
    "                    df_data['committable']['Sour gas'],\n",
    "                    df_data['committable']['Nuclear'],\n",
    "                    df_data['committable']['Wind Offshore'],\n",
    "                    df_data['committable']['Large Hydro'],\n",
    "                    df_data['committable']['Small Hydro'],\n",
    "                    df_data['committable']['Anaerobic Digestion'],\n",
    "                    df_data['committable']['EfW Incineration'],\n",
    "                    df_data['committable']['Landfill Gas'],\n",
    "                    df_data['committable']['Sewage Sludge Digestion'],\n",
    "                    df_data['committable']['Shoreline Wave'],\n",
    "                    df_data['committable']['Tidal Barrage and Tidal Stream'],\n",
    "                    df_data['committable']['Biomass (dedicated)'],\n",
    "                    df_data['committable']['Biomass (co-firing)'],\n",
    "                    df_data['committable']['Wind Onshore'],\n",
    "                    df_data['committable']['Solar Photovoltaics'],\n",
    "                    df_data['committable']['CCS Gas'],\n",
    "                    df_data['committable']['CCS Biomass'],\n",
    "                    df_data['committable']['Hydrogen']]\n",
    "\n",
    "    p_min_pu_ = [df_data['p_min_pu']['Coal'],\n",
    "                 df_data['p_min_pu']['Oil'],\n",
    "                 df_data['p_min_pu']['CCGT'],\n",
    "                 df_data['p_min_pu']['OCGT'],\n",
    "                 df_data['p_min_pu']['Sour gas'],\n",
    "                 df_data['p_min_pu']['Nuclear'],\n",
    "                 df_data['p_min_pu']['Wind Offshore'],\n",
    "                 df_data['p_min_pu']['Large Hydro'],\n",
    "                 df_data['p_min_pu']['Small Hydro'],\n",
    "                 df_data['p_min_pu']['Anaerobic Digestion'],\n",
    "                 df_data['p_min_pu']['EfW Incineration'],\n",
    "                 df_data['p_min_pu']['Landfill Gas'],\n",
    "                 df_data['p_min_pu']['Sewage Sludge Digestion'],\n",
    "                 df_data['p_min_pu']['Shoreline Wave'],\n",
    "                 df_data['p_min_pu']['Tidal Barrage and Tidal Stream'],\n",
    "                 df_data['p_min_pu']['Biomass (dedicated)'],\n",
    "                 df_data['p_min_pu']['Biomass (co-firing)'],\n",
    "                 df_data['p_min_pu']['Wind Onshore'],\n",
    "                 df_data['p_min_pu']['Solar Photovoltaics'],\n",
    "                 df_data['p_min_pu']['CCS Gas'],\n",
    "                 df_data['p_min_pu']['CCS Biomass'],\n",
    "                 df_data['p_min_pu']['Hydrogen']]\n",
    "\n",
    "    p_max_pu_ = [df_data['p_max_pu']['Coal'],\n",
    "                 df_data['p_max_pu']['Oil'],\n",
    "                 df_data['p_max_pu']['CCGT'],\n",
    "                 df_data['p_max_pu']['OCGT'],\n",
    "                 df_data['p_max_pu']['Sour gas'],\n",
    "                 df_data['p_max_pu']['Nuclear'],\n",
    "                 df_data['p_max_pu']['Wind Offshore'],\n",
    "                 df_data['p_max_pu']['Large Hydro'],\n",
    "                 df_data['p_max_pu']['Small Hydro'],\n",
    "                 df_data['p_max_pu']['Anaerobic Digestion'],\n",
    "                 df_data['p_max_pu']['EfW Incineration'],\n",
    "                 df_data['p_max_pu']['Landfill Gas'],\n",
    "                 df_data['p_max_pu']['Sewage Sludge Digestion'],\n",
    "                 df_data['p_max_pu']['Shoreline Wave'],\n",
    "                 df_data['p_max_pu']['Tidal Barrage and Tidal Stream'],\n",
    "                 df_data['p_max_pu']['Biomass (dedicated)'],\n",
    "                 df_data['p_max_pu']['Biomass (co-firing)'],\n",
    "                 df_data['p_max_pu']['Wind Onshore'],\n",
    "                 df_data['p_max_pu']['Solar Photovoltaics'],\n",
    "                 df_data['p_max_pu']['CCS Gas'],\n",
    "                 df_data['p_max_pu']['CCS Biomass'],\n",
    "                 df_data['p_max_pu']['Hydrogen']]\n",
    "\n",
    "    up_time_before_ = [df_data['up_time_before']['Coal'],\n",
    "                       df_data['up_time_before']['Oil'],\n",
    "                       df_data['up_time_before']['CCGT'],\n",
    "                       df_data['up_time_before']['OCGT'],\n",
    "                       df_data['up_time_before']['Sour gas'],\n",
    "                       df_data['up_time_before']['Nuclear'],\n",
    "                       df_data['up_time_before']['Wind Offshore'],\n",
    "                       df_data['up_time_before']['Large Hydro'],\n",
    "                       df_data['up_time_before']['Small Hydro'],\n",
    "                       df_data['up_time_before']['Anaerobic Digestion'],\n",
    "                       df_data['up_time_before']['EfW Incineration'],\n",
    "                       df_data['up_time_before']['Landfill Gas'],\n",
    "                       df_data['up_time_before']['Sewage Sludge Digestion'],\n",
    "                       df_data['up_time_before']['Shoreline Wave'],\n",
    "                       df_data['up_time_before']['Tidal Barrage and Tidal Stream'],\n",
    "                       df_data['up_time_before']['Biomass (dedicated)'],\n",
    "                       df_data['up_time_before']['Biomass (co-firing)'],\n",
    "                       df_data['up_time_before']['Wind Onshore'],\n",
    "                       df_data['up_time_before']['Solar Photovoltaics'],\n",
    "                       df_data['up_time_before']['CCS Gas'],\n",
    "                       df_data['up_time_before']['CCS Biomass'],\n",
    "                       df_data['up_time_before']['Hydrogen']]\n",
    "\n",
    "    start_up_cost_ = [df_data['start_up_cost']['Coal'],\n",
    "                      df_data['start_up_cost']['Oil'],\n",
    "                      df_data['start_up_cost']['CCGT'],\n",
    "                      df_data['start_up_cost']['OCGT'],\n",
    "                      df_data['start_up_cost']['Sour gas'],\n",
    "                      df_data['start_up_cost']['Nuclear'],\n",
    "                      df_data['start_up_cost']['Wind Offshore'],\n",
    "                      df_data['start_up_cost']['Large Hydro'],\n",
    "                      df_data['start_up_cost']['Small Hydro'],\n",
    "                      df_data['start_up_cost']['Anaerobic Digestion'],\n",
    "                      df_data['start_up_cost']['EfW Incineration'],\n",
    "                      df_data['start_up_cost']['Landfill Gas'],\n",
    "                      df_data['start_up_cost']['Sewage Sludge Digestion'],\n",
    "                      df_data['start_up_cost']['Shoreline Wave'],\n",
    "                      df_data['start_up_cost']['Tidal Barrage and Tidal Stream'],\n",
    "                      df_data['start_up_cost']['Biomass (dedicated)'],\n",
    "                      df_data['start_up_cost']['Biomass (co-firing)'],\n",
    "                      df_data['start_up_cost']['Wind Onshore'],\n",
    "                      df_data['start_up_cost']['Solar Photovoltaics'],\n",
    "                      df_data['start_up_cost']['CCS Gas'],\n",
    "                      df_data['start_up_cost']['CCS Biomass'],\n",
    "                      df_data['start_up_cost']['Hydrogen']]\n",
    "\n",
    "    df.loc[:, 'marginal_cost'] = np.select(conditions, marg_cos)\n",
    "    df.loc[:, 'committable'] = np.select(conditions, committable_)\n",
    "    df.loc[:, 'committable'] = df['committable'].astype('bool')\n",
    "    df.loc[:, 'min_up_time'] = np.select(conditions, min_up_time_) / (time_step * 60)\n",
    "    df.loc[:, 'min_up_time'] = df['min_up_time'].astype('int')\n",
    "    # df.loc[:, 'min_up_time'] = 0\n",
    "    df.loc[:, 'min_down_time'] = np.select(conditions, min_down_time_) / (time_step * 60)\n",
    "    df.loc[:, 'min_down_time'] = df['min_down_time'].astype('int')\n",
    "    # df.loc[:, 'min_down_time'] = 0\n",
    "    # need to ensure the capacity is a float and not a string\n",
    "    df.loc[:, 'p_nom'] = pd.to_numeric(df[\"p_nom\"], downcast=\"float\")\n",
    "    df.loc[:, 'ramp_limit_up'] = np.select(conditions, ramp_limit_up_) * (time_step * 60) / 100\n",
    "    df.loc[:, 'ramp_limit_up'].values[df['ramp_limit_up'].values > 1.0] = 1.0\n",
    "    df.loc[:, 'ramp_limit_down'] = np.select(conditions, ramp_limit_down_) * (time_step * 60) / 100\n",
    "    df.loc[:, 'ramp_limit_down'].values[df['ramp_limit_down'].values > 1.0] = 1.0\n",
    "    df.loc[:, 'p_min_pu'] = np.select(conditions, p_min_pu_) / 100\n",
    "    df.loc[:, 'p_max_pu'] = np.select(conditions, p_max_pu_) / 100\n",
    "    # df.loc[:, 'p_min_pu'] = 0\n",
    "    df.loc[:, 'up_time_before'] = np.select(conditions, up_time_before_)\n",
    "    df.loc[:, 'up_time_before'] = df['up_time_before'].astype('int')\n",
    "    df.loc[:, 'start_up_cost'] = np.select(conditions, start_up_cost_)\n",
    "    df.loc[:, 'start_up_cost'] *= df['p_nom']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_coal_p_nom(year):\n",
    "    # read in phase out of coal dates\n",
    "    file = '../data/power stations/coal_phase_out_dates.csv'\n",
    "    df = pd.read_csv(file, index_col=1)\n",
    "    df.index = pd.to_datetime(df.index, format=\"%d/%m/%Y\")\n",
    "    end_date = str(year) + '-01-01'\n",
    "    filtered_df = df.loc[:end_date]\n",
    "    pp_to_remove = filtered_df.name.values\n",
    "    # need to remove this because it deletes CCS biomass, and actually is zero anyway\n",
    "    pp_to_remove = pp_to_remove[pp_to_remove != 'West Burton']\n",
    "\n",
    "    # get generators dataframe with p_noms to be scaled\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "\n",
    "    # error occurs because deleting PV farm with Ratcliffe in name\n",
    "    # just add in later\n",
    "    try:\n",
    "        PV_ratcliffe = generators.loc[['Ld NW Of Ratcliffe House Farm']]\n",
    "        PV_ratcliffe_UC = generators_UC.loc[['Ld NW Of Ratcliffe House Farm']]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i in range(len(pp_to_remove)):\n",
    "        # remove those who are not in date\n",
    "        # just look at the coal generators\n",
    "        # keep PV row with Ratcliffe in name\n",
    "\n",
    "        generators = generators[~generators.index.str.contains(pp_to_remove[i])]\n",
    "        generators_UC = generators_UC[~generators_UC.index.str.contains(pp_to_remove[i])]\n",
    "\n",
    "    # append the PV farm with Ratcliffe in name, only in years Ratcliffe is removed\n",
    "    try:\n",
    "        if year > 2024:\n",
    "            generators = pd.concat([generators, PV_ratcliffe])\n",
    "            generators_UC = pd.concat([generators_UC, PV_ratcliffe_UC])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if year > 2022:\n",
    "        generators['p_nom']['West Burton'] = 0\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a954f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_gas_p_nom(year, scenario, tech, FES):\n",
    "    # going to scale the OCGT and CCGT based on FES\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    if tech == 'OCGT' or tech == 'CCGT':\n",
    "        # get generators dataframe with p_noms to be scaled\n",
    "        path = 'LOPF_data/generators.csv'\n",
    "        generators = pd.read_csv(path, index_col=0)\n",
    "        gen_tech = generators.loc[generators['type'] == tech]\n",
    "\n",
    "        path_UC = 'UC_data/generators.csv'\n",
    "        generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "        gen_tech_UC = generators_UC.loc[generators_UC['type'] == tech]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to remove the original tech\n",
    "    # print(generators.loc[generators['type'] == tech])\n",
    "    generators = generators[~generators.type.str.contains(tech)]\n",
    "    generators_UC = generators_UC[~generators_UC.type.str.contains(tech)]\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech])\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC])\n",
    "\n",
    "    # print(generators.loc[generators['type'] == tech])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_nuclear_p_nom(year, scenario, FES, networkmodel='Reduced'):\n",
    "    if networkmodel == 'Reduced':\n",
    "        from distance_calculator import map_to_bus as map_to\n",
    "    elif networkmodel == 'Zonal':\n",
    "        from allocate_to_zone import map_to_zone as map_to\n",
    "    # read in phase out of nuclear dates\n",
    "    file = '../data/power stations/nuclear_phase_out_dates.csv'\n",
    "    df = pd.read_csv(file, index_col=1)\n",
    "    df.index = pd.to_datetime(df.index, format=\"%d/%m/%Y\")\n",
    "    end_date = str(year) + '/01/01'\n",
    "    filtered_df = df.sort_index().loc[:end_date]\n",
    "    pp_to_remove = filtered_df.name.values\n",
    "\n",
    "    # get generators dataframe with p_noms to be scaled\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    # print(generators.loc[generators['carrier'] == 'Nuclear'])\n",
    "    for i in range(len(pp_to_remove)):\n",
    "        # remove those who are not in date\n",
    "        generators = generators[~generators.index.str.contains(pp_to_remove[i])]\n",
    "        generators_UC = generators_UC[~generators_UC.index.str.contains(pp_to_remove[i])]\n",
    "    # print(generators.loc[generators['carrier'] == 'Nuclear'])\n",
    "\n",
    "    # read in new nuclear power plant data\n",
    "    file = '../data/power stations/nuclear_new_dates.csv'\n",
    "    df = pd.read_csv(file, index_col=1)\n",
    "    df.index = pd.to_datetime(df.index\n",
    "                              )\n",
    "    to_date = str(year) + '-01-01'\n",
    "    filtered_df = df.loc[:to_date]\n",
    "    pp_to_add = filtered_df.name.values\n",
    "    # print(pp_to_add)\n",
    "    # print(generators.loc[generators['carrier'] == 'Nuclear'])\n",
    "    generators2 = pd.read_csv(path, index_col=0)\n",
    "    new_nuclear = generators2.loc[generators2['carrier'] == 'Nuclear'].iloc[[0]]\n",
    "    # print(new_nuclear)\n",
    "    df_new_nuclear = pd.DataFrame()\n",
    "    for i in range(len(pp_to_add)):\n",
    "        # add new generators\n",
    "        # template\n",
    "        new_nuclear = generators2.loc[generators2['carrier'] == 'Nuclear'].iloc[[0]]\n",
    "        p_nom = filtered_df.loc[filtered_df['name'] == pp_to_add[i]]['p_nom'].values[0]\n",
    "        new_nuclear.loc[:, 'p_nom'] = p_nom\n",
    "        new_nuclear.loc[:, 'type'] = 'PWR'\n",
    "        new_nuclear.index = [pp_to_add[i]]\n",
    "        new_nuclear['x'] = filtered_df.loc[filtered_df['name'] == pp_to_add[i]]['x'].values[0]\n",
    "        new_nuclear['y'] = filtered_df.loc[filtered_df['name'] == pp_to_add[i]]['y'].values[0]\n",
    "        # need to map to bus\n",
    "        new_nuclear['bus'] = map_to(new_nuclear)\n",
    "        df_new_nuclear = pd.concat([df_new_nuclear, new_nuclear])\n",
    "\n",
    "    # only append to generators dataframe if there are rows in df_new_nuclear\n",
    "    if not df_new_nuclear.empty:\n",
    "        df_new_nuclear.drop(columns=['x', 'y'], inplace=True)\n",
    "        # now add new nuclear to generators df\n",
    "        generators = pd.concat([generators, df_new_nuclear])\n",
    "        generators_UC = pd.concat([generators_UC, df_new_nuclear])\n",
    "        # print(generators.loc[generators['carrier'] == 'Nuclear'])\n",
    "\n",
    "    # this gets us to 2030, but want to scale the old nuclear sites\n",
    "    # for > 2030\n",
    "    if year > 2030:\n",
    "        tech = 'Nuclear'\n",
    "        future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "        tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "        tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "        # get generators dataframe with p_noms to be scaled\n",
    "        path = 'LOPF_data/generators.csv'\n",
    "        generators3 = pd.read_csv(path, index_col=0)\n",
    "        gen_tech = generators3.loc[generators3['carrier'] == tech]\n",
    "\n",
    "        path_UC = 'UC_data/generators.csv'\n",
    "        generators_UC3 = pd.read_csv(path_UC, index_col=0)\n",
    "        gen_tech_UC = generators_UC3.loc[generators_UC3['carrier'] == tech]\n",
    "\n",
    "        # then consider what scaling factor is required\n",
    "        scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "\n",
    "        # scale the p_noms of the RES generators\n",
    "        for g in gen_tech.index:\n",
    "            gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "            gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "        # write new generators.csv file\n",
    "        # save the dataframes to csv\n",
    "        # need to remove the original tech\n",
    "        generators3 = generators3[~generators3.carrier.str.contains(tech)]\n",
    "        generators_UC3 = generators_UC3[~generators_UC3.carrier.str.contains(tech)]\n",
    "        # then add the new p_nom tech\n",
    "        generators3 = pd.concat([generators3, gen_tech])\n",
    "        generators_UC3 = pd.concat([generators_UC3, gen_tech_UC])\n",
    "\n",
    "    else:\n",
    "        generators_UC3 = generators_UC.copy()\n",
    "        generators3 = generators.copy()\n",
    "\n",
    "    generators_UC3.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators3.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_oil_p_nom(year, scenario, FES):\n",
    "    tech = 'Oil'\n",
    "    # going to scale the oil based on FES\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    # get generators dataframe with p_noms to be scaled\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['carrier'] == tech]\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['carrier'] == tech]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to remove the original tech\n",
    "    # print(generators.loc[generators['carrier'] == tech])\n",
    "    generators = generators[~generators.carrier.str.contains(tech)]\n",
    "    generators_UC = generators_UC[~generators_UC.carrier.str.contains(tech)]\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech])\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC])\n",
    "\n",
    "    # print(generators.loc[generators['carrier'] == tech])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeeef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_waste_p_nom(year, scenario, FES):\n",
    "    tech = 'Waste'\n",
    "    # going to scale the oil based on FES\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    tech = 'EfW Incineration'\n",
    "    # get generators dataframe with p_noms to be scaled\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['carrier'] == tech]\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['carrier'] == tech]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to remove the original tech\n",
    "    # print(generators.loc[generators['carrier'] == tech])\n",
    "    generators = generators[~generators.carrier.str.contains(tech)]\n",
    "    generators_UC = generators_UC[~generators_UC.carrier.str.contains(tech)]\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech])\n",
    "\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC])\n",
    "\n",
    "    # print(generators.loc[generators['carrier'] == tech])\n",
    "    generators['carrier'] = generators['carrier'].replace({'EfW Incineration':'Waste'})\n",
    "    generators_UC['carrier'] = generators_UC['carrier'].replace({'EfW Incineration':'Waste'})\n",
    "    generators['type'] = generators['type'].replace({'EfW Incineration':'Waste'})\n",
    "    generators_UC['type'] = generators_UC['type'].replace({'EfW Incineration':'Waste'})\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_gas_CCS(year, scenario, FES):\n",
    "    tech = 'CCS Gas'\n",
    "    # going to scale the existing gas sites based on FES\n",
    "    # but add as new tech\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    # CCS sometimes coming out as nan, replace with zero\n",
    "    if np.isnan(tech_cap_FES):\n",
    "        tech_cap_FES = 0.0\n",
    "\n",
    "    tech_ = 'CCGT'\n",
    "    # get CCGT generators as they are in year\n",
    "    # need to ensure doing this function before scaling gas\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['type'] == tech_]\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['type'] == tech_]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = tech_cap_FES / tech_cap_year\n",
    "\n",
    "    # don't want to scale original CCGT so need copy\n",
    "    gen_tech2 = gen_tech.copy()\n",
    "    gen_tech_UC2 = gen_tech_UC.copy()\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech2.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC2.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to change the carrier and type before appending\n",
    "    # also note that not removing CCGT generators\n",
    "    gen_tech2.loc[:, 'type'] = 'CCS Gas'\n",
    "    gen_tech2.loc[:, 'carrier'] = 'CCS Gas'\n",
    "    gen_tech_UC2.loc[:, 'type'] = 'CCS Gas'\n",
    "    gen_tech_UC2.loc[:, 'carrier'] = 'CCS Gas'\n",
    "    # need to modify names by adding CCS Gas\n",
    "    gen_tech2.index += ' CCS Gas'\n",
    "    gen_tech_UC2.index += ' CCS Gas'\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech2])\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC2])\n",
    "\n",
    "    # print(generators.loc[generators['carrier'] == tech])\n",
    "    # print(generators.loc[generators['carrier'] == 'Natural Gas'])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_biomass_CCS(year, scenario, FES):\n",
    "    tech = 'CCS Biomass'\n",
    "    # going to scale the existing gas sites based on FES\n",
    "    # but add as new tech\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    # CCS biomass sometimes coming out as nan, replace with zero\n",
    "    if np.isnan(tech_cap_FES):\n",
    "        tech_cap_FES = 0.0\n",
    "\n",
    "    tech_ = 'CCGT'\n",
    "    # get CCGT generators as they are in year\n",
    "    # need to ensure doing this function before scaling gas\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['type'] == tech_]\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['type'] == tech_]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = tech_cap_FES / tech_cap_year\n",
    "\n",
    "    # don't want to scale original CCGT so need copy\n",
    "    gen_tech2 = gen_tech.copy()\n",
    "    gen_tech_UC2 = gen_tech_UC.copy()\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech2.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC2.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to change the carrier and type before appending\n",
    "    # also note that not removing CCGT generators\n",
    "    gen_tech2.loc[:, 'type'] = 'CCS Biomass'\n",
    "    gen_tech2.loc[:, 'carrier'] = 'CCS Biomass'\n",
    "    gen_tech_UC2.loc[:, 'type'] = 'CCS Biomass'\n",
    "    gen_tech_UC2.loc[:, 'carrier'] = 'CCS Biomass'\n",
    "    # need to modify names by adding CCS Biomass\n",
    "    gen_tech2.index += ' CCS Biomass'\n",
    "    gen_tech_UC2.index += ' CCS Biomass'\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech2])\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC2])\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_hydrogen(year, scenario, FES):\n",
    "    tech = 'Hydrogen'\n",
    "    # going to scale the existing gas sites based on FES\n",
    "    # but add as new tech\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "    tech_cap_year = future_capacities_dict['tech_cap_year']\n",
    "    tech_cap_FES = future_capacities_dict['tech_cap_FES']\n",
    "\n",
    "    # hydrogen sometimes coming out as nan, replace with zero\n",
    "    if np.isnan(tech_cap_FES):\n",
    "        tech_cap_FES = 0.0\n",
    "\n",
    "    tech_ = 'CCGT'\n",
    "    # get CCGT generators as they are in year\n",
    "    # need to ensure doing this function before scaling gas\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    generators = pd.read_csv(path, index_col=0)\n",
    "    gen_tech = generators.loc[generators['type'] == tech_]\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    generators_UC = pd.read_csv(path_UC, index_col=0)\n",
    "    gen_tech_UC = generators_UC.loc[generators_UC['type'] == tech_]\n",
    "\n",
    "    # then consider what scaling factor is required\n",
    "    scaling_factor = round(tech_cap_FES / tech_cap_year, 2)\n",
    "    # print(scaling_factor, 'hydrogen scaling factor')\n",
    "\n",
    "    # don't want to scale original CCGT so need copy\n",
    "    gen_tech2 = gen_tech.copy()\n",
    "    gen_tech_UC2 = gen_tech_UC.copy()\n",
    "\n",
    "    # scale the p_noms of the RES generators\n",
    "    for g in gen_tech.index:\n",
    "        gen_tech2.loc[g, 'p_nom'] *= scaling_factor\n",
    "        gen_tech_UC2.loc[g, 'p_nom'] *= scaling_factor\n",
    "\n",
    "    # write new generators.csv file\n",
    "    # save the dataframes to csv\n",
    "    # need to change the carrier and type before appending\n",
    "    # also note that not removing CCGT generators\n",
    "    gen_tech2.loc[:, 'type'] = 'Hydrogen'\n",
    "    gen_tech2.loc[:, 'carrier'] = 'Hydrogen'\n",
    "    gen_tech_UC2.loc[:, 'type'] = 'Hydrogen'\n",
    "    gen_tech_UC2.loc[:, 'carrier'] = 'Hydrogen'\n",
    "    # need to modify names by adding Hydrogen\n",
    "    gen_tech2.index += ' Hydrogen'\n",
    "    gen_tech_UC2.index += ' Hydrogen'\n",
    "    # then add the new p_nom tech\n",
    "    generators = pd.concat([generators, gen_tech2])\n",
    "\n",
    "    generators_UC = pd.concat([generators_UC, gen_tech_UC2])\n",
    "\n",
    "\n",
    "    generators_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    generators.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_capacity(year, tech, scenario, FES):\n",
    "\n",
    "    df_pp = read_power_stations_data(year)\n",
    "    if tech == 'Nuclear' or tech == 'Oil':\n",
    "        df_pp = df_pp[df_pp.Fuel.str.contains(tech)]\n",
    "        tech_cap_year = df_pp['Installed Capacity (MW)'].sum() / 1000\n",
    "    elif tech == 'CCGT' or tech == 'OCGT':\n",
    "        df_pp = df_pp[df_pp.Technology.str.contains(tech)]\n",
    "        tech_cap_year = df_pp['Installed Capacity (MW)'].sum() / 1000\n",
    "    elif tech == 'Waste':\n",
    "        df_pp = renewables.read_non_dispatchable_continuous(year)\n",
    "        df_pp = df_pp[df_pp.type.str.contains('EfW Incineration')]\n",
    "        # this number excludes EFW CHP but FES includes it\n",
    "        # which explains discrepancy\n",
    "        tech_cap_year = df_pp['p_nom'].sum() / 1000\n",
    "    elif tech == 'CCS Gas' or tech == 'CCS Biomass' or tech == 'Hydrogen':\n",
    "        df_pp = df_pp[df_pp.Technology.str.contains('CCGT')]\n",
    "        tech_cap_year = df_pp['Installed Capacity (MW)'].sum() / 1000\n",
    "\n",
    "    if tech == 'CCGT':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        df_FES = df_FES[df_FES.SubType.str.contains('CCGT', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "        # print(df_FES)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "        df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "        df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "        df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "        df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "        df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "        df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "        df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "        df_FES_ST.index = ['System Transformation']\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "        if FES == 2022:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'OCGT':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        df_FES = df_FES[df_FES.SubType.str.contains('OCGT', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "        # print(df_FES)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "        df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "        df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "        df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "        df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "        df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "        df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "        df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "        df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "        if FES == 2022:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'Nuclear':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES = df_FES[df_FES.Type.str.contains('Nuclear', case=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "    elif tech == 'Oil':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        # df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES = df_FES[df_FES.Type.str.contains('Other Thermal', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "        df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "        df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "        df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "        df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "        df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "        df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "        df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "        df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "        if FES == 2022:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'Waste':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        # df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES = df_FES[df_FES.Type.str.contains('Waste', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)]\n",
    "        df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "        df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "        df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "        df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "        df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "        df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "        df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "        df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Steady Progression', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Steady Progression']\n",
    "        if FES == 2022:\n",
    "            df_FES_SP = df_FES[df_FES.index.str.contains('Falling Short', case=False)]\n",
    "            df_FES_SP = pd.DataFrame(df_FES_SP.sum(numeric_only=True)).T\n",
    "            df_FES_SP.index = ['Falling Short']\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'CCS Gas':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        df_FES = df_FES[df_FES.SubType.str.contains('CCS Gas', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "        if FES == 2021:\n",
    "            df_LTW = pd.DataFrame(0, columns=df_FES.columns, index=['Leading the Way'])\n",
    "            df_CT = pd.DataFrame(0, columns=df_FES.columns, index=['Consumer Transformation'])\n",
    "            df_FES = pd.concat([df_FES, df_LTW, df_CT], sort=True)\n",
    "\n",
    "    elif tech == 'CCS Biomass':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        df_FES = df_FES[df_FES.SubType.str.contains('CCS Biomass', case=False, na=False)]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = pd.DataFrame(0, columns=df_FES.columns, index=['Steady Progression'])\n",
    "            df_FES = pd.concat([df_FES, df_FES_SP], sort=True)\n",
    "\n",
    "    elif tech == 'Hydrogen':\n",
    "        if FES == 2021:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2021/FES 2021 Data Workbook V04.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == 2022:\n",
    "            df_FES = pd.read_excel(\n",
    "                '../data/FES2022/FES2022 Workbook V4.xlsx',\n",
    "                sheet_name='ES1', header=9, index_col=1)\n",
    "        elif FES == None:\n",
    "            raise Exception('Please choose a FES year.')\n",
    "        # df_FES.dropna(axis='rows', inplace=True)\n",
    "        df_FES = df_FES[df_FES.SubType.str.contains('Hydrogen', case=False, na=False)]\n",
    "        # df_FES = df_FES[~df_FES.SubType.str.contains('Hydrogen CHP')]\n",
    "        df_FES = df_FES[~df_FES.Variable.str.contains('Generation')]\n",
    "        cols = [0, 1, 2, 3, 4]\n",
    "        df_FES.drop(df_FES.columns[cols], axis=1, inplace=True)\n",
    "        df_FES = df_FES.fillna(0)\n",
    "\n",
    "        df_FES_LTW = df_FES[df_FES.index.str.contains('Leading The Way', case=False)].reset_index()\n",
    "        if FES == 2022:\n",
    "            df_FES_LTW = pd.DataFrame(df_FES_LTW.sum(numeric_only=True)).T\n",
    "            df_FES_LTW.index = ['Leading the Way']\n",
    "\n",
    "        if FES == 2022:\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        if FES == 2021:\n",
    "            df_FES_CT = df_FES[df_FES.index.str.contains('Consumer Transformation', case=False)]\n",
    "            df_FES_CT = pd.DataFrame(df_FES_CT.sum(numeric_only=True)).T\n",
    "            df_FES_CT.index = ['Consumer Transformation']\n",
    "\n",
    "            df_FES_ST = df_FES[df_FES.index.str.contains('System Transformation', case=False)]\n",
    "            df_FES_ST = pd.DataFrame(df_FES_ST.sum(numeric_only=True)).T\n",
    "            df_FES_ST.index = ['System Transformation']\n",
    "\n",
    "        if FES == 2021:\n",
    "            df_FES_SP = pd.DataFrame(0, columns=df_FES.columns, index=['Steady Progression'])\n",
    "        if FES == 2022:\n",
    "            df_FES_SP = pd.DataFrame(0, columns=df_FES.columns, index=['Falling Short'])\n",
    "\n",
    "        df_FES = pd.concat([df_FES_SP, df_FES_LTW, df_FES_CT, df_FES_ST])\n",
    "\n",
    "    elif tech == 'Marine':\n",
    "        pass\n",
    "    \n",
    "    date = str(year) + '-01-01'\n",
    "\n",
    "    if scenario == 'Leading The Way':\n",
    "        try:\n",
    "            scenario = 'Leading the Way'\n",
    "            try:\n",
    "                tech_cap_FES = float(df_FES.loc[scenario, date]) / 1000.\n",
    "            except:\n",
    "                tech_cap_FES = float(df_FES.loc[scenario, year]) / 1000.\n",
    "        except:\n",
    "            scenario = 'Leading The Way'\n",
    "            try:\n",
    "                tech_cap_FES = float(df_FES.loc[scenario, date]) / 1000.\n",
    "            except:\n",
    "                tech_cap_FES = float(df_FES.loc[scenario, year]) / 1000.\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, date]) / 1000.\n",
    "        except:\n",
    "            tech_cap_FES = float(df_FES.loc[scenario, year]) / 1000.\n",
    "\n",
    "    if np.isnan(tech_cap_FES):\n",
    "        tech_cap_FES = 0.0\n",
    "    capacity_dict = {'tech_cap_year': tech_cap_year,\n",
    "                     'tech_cap_FES': tech_cap_FES}\n",
    "\n",
    "    return capacity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751de141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generators_p_max_pu(start, end, freq, year, FES=None, year_baseline=None, scenario=None):\n",
    "    \"\"\"writes the generators p_max_pu csv file\n",
    "\n",
    "    writes the timeseries maximum power output file for the\n",
    "    non-dispatchable renewable generators\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str\n",
    "        start of simulation\n",
    "    end : str\n",
    "        end of simulation\n",
    "    freq : str\n",
    "        frequency of timestep, only 'h' or '0.5h' allowed currently\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    # GENERATORS-P_MAX_PU FILE\n",
    "\n",
    "    # WIND OFFSHORE\n",
    "\n",
    "    # file = 'data/renewables/' + str(year) + '/offshore_time_series_norm.pkl'\n",
    "    # df = pd.read_pickle(file)\n",
    "    # fix according to operational dates\n",
    "    tech = 'Wind Offshore'\n",
    "    if year <= 2020:\n",
    "        df_offshore = renewables.historical_RES_timeseries(year, tech, future=False)['norm']\n",
    "    elif year > 2020:\n",
    "        df_offshore = renewables.future_offshore_timeseries(year, year_baseline, scenario, FES)['norm']\n",
    "    df_offshore = df_offshore.loc[start:end]\n",
    "\n",
    "    if freq == '0.5H':\n",
    "        # resample to half hourly timesteps\n",
    "        df_offshore = df_offshore.resample(freq).interpolate('polynomial', order=2)\n",
    "        # need to add a row at end\n",
    "        # the data being passed is the values of the last row\n",
    "        # the tail function is used to get the last index value\n",
    "        df_offshore_new = pd.DataFrame(\n",
    "            data=[df_offshore.loc[df_offshore.tail(1).index.values].values[0]],\n",
    "            columns=df_offshore.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_offshore = pd.concat([df_offshore, df_offshore_new], sort=False)\n",
    "\n",
    "    # name the index\n",
    "    df_offshore.index.name = 'name'\n",
    "    df_offshore.index = pd.to_datetime(df_offshore.index)\n",
    "\n",
    "    # WIND ONSHORE\n",
    "\n",
    "    # file = 'data/renewables/' + str(year) + '/onshore_time_series_norm.pkl'\n",
    "    # df_onshore = pd.read_pickle(file)\n",
    "    # fix according to operational dates\n",
    "    tech = 'Wind Onshore'\n",
    "    if year > 2020:\n",
    "        df_onshore = renewables.historical_RES_timeseries(year_baseline, tech, future=True)['norm']\n",
    "        # edit the years on the start and end to match the baseline year\n",
    "        start = str(year_baseline) + start[4:]\n",
    "        end = str(year_baseline) + end[4:]\n",
    "\n",
    "    elif year <= 2020:\n",
    "        # either overwrite optional argument or define it as equal to year\n",
    "        df_onshore = renewables.historical_RES_timeseries(year, tech, future=False)['norm']\n",
    "\n",
    "    df_onshore = df_onshore.loc[start:end]\n",
    "\n",
    "    if freq == '0.5h':\n",
    "        # resample to half hourly timesteps\n",
    "        df_onshore = df_onshore.resample(freq).interpolate('polynomial', order=2)\n",
    "        # need to add a row at end\n",
    "        # the data being passed is the values of the last row\n",
    "        # the tail function is used to get the last index value\n",
    "        df_new_onshore = pd.DataFrame(\n",
    "            data=[df_onshore.loc[df_onshore.tail(1).index.values].values[0]],\n",
    "            columns=df_onshore.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_onshore = pd.concat([df_onshore, df_new_onshore], sort=False)\n",
    "\n",
    "    # name the index\n",
    "    df_onshore.index.name = 'name'\n",
    "    df_onshore.index = pd.to_datetime(df_onshore.index)\n",
    "\n",
    "    # check if baseline year is a leap year and simulated year is not and remove 29th Feb\n",
    "    if year_baseline is not None:\n",
    "        if year_baseline % 4 == 0:\n",
    "            # and the year modelled is also not a leap year\n",
    "            if year % 4 != 0:\n",
    "                # remove 29th Feb\n",
    "                df_onshore = df_onshore[~((df_onshore.index.month == 2) & (df_onshore.index.day == 29))]\n",
    "\n",
    "    df_onshore.index = df_offshore.index\n",
    "                \n",
    "    # PV\n",
    "\n",
    "    # file = 'data/renewables/' + str(year) + '/PV_time_series_norm.pkl'\n",
    "    # df_PV = pd.read_pickle(file)\n",
    "    # fix according to operational dates\n",
    "    tech = 'Solar Photovoltaics'\n",
    "    if year > 2020:\n",
    "        df_PV = renewables.historical_RES_timeseries(year_baseline, tech, future=True)['norm']\n",
    "        # edit the years on the start and end to match the baseline year\n",
    "        # will change for onshore but including here to be explicit\n",
    "        # incase no onshore wind\n",
    "        start = str(year_baseline) + start[4:]\n",
    "        end = str(year_baseline) + end[4:]\n",
    "\n",
    "    elif year <= 2020:\n",
    "        # note that use baseline year normalised distribution for RES timeseries for future\n",
    "        # baseline year equated to modelled year for historical\n",
    "        df_PV = renewables.historical_RES_timeseries(year, tech, future=False)['norm']\n",
    "\n",
    "    df_PV = df_PV.loc[start:end]\n",
    "\n",
    "    # resample to half hourly timesteps\n",
    "    if freq == '0.5h':\n",
    "        df_PV = df_PV.resample(freq).interpolate('polynomial', order=1)\n",
    "        # need to add a row at end\n",
    "        # the data being passed is the values of the last row\n",
    "        # the tail function is used to get the last index value\n",
    "        df_new_PV = pd.DataFrame(\n",
    "            data=[df_PV.loc[df_PV.tail(1).index.values].values[0]],\n",
    "            columns=df_PV.columns,\n",
    "            index=[end])\n",
    "        # add to existing dataframe\n",
    "        df_PV = pd.concat([df_PV, df_new_PV], sort=False)\n",
    "\n",
    "    # name the index\n",
    "    df_PV.index.name = 'name'\n",
    "    df_PV.index = pd.to_datetime(df_PV.index)\n",
    "\n",
    "    # check if baseline year is a leap year and simulated year is not and remove 29th Feb\n",
    "    if year_baseline is not None:\n",
    "        if year_baseline % 4 == 0:\n",
    "            # and the year modelled is also not a leap year\n",
    "            if year % 4 != 0:\n",
    "                # remove 29th Feb\n",
    "                df_PV = df_PV[~((df_PV.index.month == 2) & (df_PV.index.day == 29))]\n",
    "\n",
    "    df_PV.index = df_offshore.index\n",
    "\n",
    "    # HYDRO\n",
    "    # hydro data is between 2015-02-22 and 2020-12-31\n",
    "    # if dates are before then ATM use 2016 data\n",
    "    tech = 'Hydro'\n",
    "    if year > 2020:\n",
    "        # edit the years on the start and end to match the baseline year\n",
    "        # will change for onshore but including here to be explicit\n",
    "        # incase no onshore wind\n",
    "        start = str(year_baseline) + start[4:]\n",
    "        end = str(year_baseline) + end[4:]\n",
    "\n",
    "        df_hydro1 = renewables.read_hydro_time_series(year_baseline)['time_series_norm']\n",
    "\n",
    "    elif year <= 2020:\n",
    "        df_hydro1 = renewables.read_hydro_time_series(year)['time_series_norm']\n",
    "\n",
    "    df_hydro = df_hydro1.loc[start:end]\n",
    "    # some February values for 2015 are being overwritten here...\n",
    "    # might be a better solution to this out there\n",
    "    if df_hydro.empty or start[:7] == '2015-02':\n",
    "        start = '2016' + start[4:]\n",
    "        end = '2016' + end[4:]\n",
    "        df_hydro = df_hydro1.loc[start:end]\n",
    "\n",
    "    df_hydro = df_hydro.resample(freq).mean()\n",
    "    if freq == 'h':\n",
    "        df_hydro = df_hydro.resample(freq).interpolate('polynomial', order=1)\n",
    "        # df_hydro = df_hydro.iloc[:-1, :]\n",
    "        # # need to add a row at end\n",
    "        # # the data being passed is the values of the last row\n",
    "        # # the tail function is used to get the last index value\n",
    "        # df_new_hydro = pd.DataFrame(\n",
    "        #     data=[df_hydro.loc[df_hydro.tail(1).index.values].values[0]],\n",
    "        #     columns=df_hydro.columns,\n",
    "        #     index=[end])\n",
    "        # # add to existing dataframe\n",
    "        # df_hydro = df_hydro.append(df_new_hydro, sort=False)\n",
    "\n",
    "    if year > 2020:\n",
    "        # year modelled is not a leap year \n",
    "        if year % 4 != 0:\n",
    "            # remove 29th Feb\n",
    "            df_hydro = df_hydro[~((df_hydro.index.month == 2) & (df_hydro.index.day == 29))]\n",
    "        # however if modelled year is a leap year and the baseline is not then also need to remove\n",
    "        if year % 4 == 0 and year_baseline % 4 != 0:\n",
    "            # remove 29th Feb\n",
    "            df_hydro = df_hydro[~((df_hydro.index.month == 2) & (df_hydro.index.day == 29))]\n",
    "\n",
    "    df_hydro.index = df_offshore.index\n",
    "    \n",
    "    # MARINE TECHNOLOGIES\n",
    "\n",
    "    # want to join the three dataframes together\n",
    "    dfs = [df_offshore, df_onshore, df_PV, df_hydro]\n",
    "    # if year <= 2020:\n",
    "    #     dfs = unify_index(dfs, freq)\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # make sure there are no missing values\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # make sure there are no negative values\n",
    "    df[df < 0] = 0\n",
    "    df[df > 1] = 1\n",
    "    # fix the column names\n",
    "    df.columns = df.columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df.columns = df.columns.astype(str).str.replace(u'\\xa0', '')\n",
    "    df.columns = df.columns.astype(str).str.replace('ì', 'i')\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # want to ensure no duplicate names\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    # rename the columns with the cols list.\n",
    "    df.columns = cols\n",
    "\n",
    "    df.to_csv('UC_data/generators-p_max_pu.csv', header=True)\n",
    "    df.to_csv('LOPF_data/generators-p_max_pu.csv', header=True)\n",
    "    # this fixes the output from this\n",
    "    # df.to_csv('UC_data/generators-p_min_pu.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_p_nom(year, time_step, scenario, FES, networkmodel='Reduced'):\n",
    "    # need to do CCS first as they are scaling based on\n",
    "    # 2020 data... make sure to do this before scaling gas p_nom\n",
    "    future_gas_CCS(year, scenario, FES)\n",
    "    future_biomass_CCS(year, scenario, FES)\n",
    "    future_hydrogen(year, scenario, FES)\n",
    "    renewables.scale_biomass_p_nom(year, scenario, FES)\n",
    "    future_coal_p_nom(year)\n",
    "    future_gas_p_nom(year, scenario, 'CCGT', FES)\n",
    "    future_gas_p_nom(year, scenario, 'OCGT', FES)\n",
    "    future_nuclear_p_nom(year, scenario, FES, networkmodel=networkmodel)\n",
    "    future_oil_p_nom(year, scenario, FES)\n",
    "    future_waste_p_nom(year, scenario, FES)\n",
    "    renewables.future_RES_scale_p_nom(year, 'Wind Onshore', scenario, FES)\n",
    "    renewables.future_RES_scale_p_nom(year, 'Solar Photovoltaics', scenario, FES)\n",
    "    renewables.future_RES_scale_p_nom(year, 'Hydro', scenario, FES)\n",
    "\n",
    "    # ensure all generator data is added\n",
    "    df_UC = pd.read_csv('UC_data/generators.csv', index_col=0)\n",
    "    df_LOPF = pd.read_csv('LOPF_data/generators.csv', index_col=0)\n",
    "\n",
    "    # run additional data for both UC and LOPF\n",
    "    df_UC = generator_additional_data(df_UC, time_step)\n",
    "    df_LOPF = generator_additional_data(df_LOPF, time_step)\n",
    "    # remove the unit committent constraints\n",
    "    df_LOPF = df_LOPF.drop(\n",
    "        columns=['committable', 'min_up_time', 'min_down_time',\n",
    "                 'p_min_pu', 'up_time_before', 'start_up_cost'])\n",
    "\n",
    "    # remove old generator types\n",
    "    # df_LOPF = df_LOPF.loc[df_LOPF['carrier'] == 'Hydrogen']\n",
    "    df_LOPF = df_LOPF[~df_LOPF.type.str.contains('Landfill Gas')]\n",
    "    df_UC = df_UC[~df_UC.type.str.contains('Landfill Gas')]    \n",
    "    df_LOPF = df_LOPF[~df_LOPF.type.str.contains('Sewage Sludge Digestion')]\n",
    "    df_UC = df_UC[~df_UC.type.str.contains('Sewage Sludge Digestion')]\n",
    "    df_LOPF = df_LOPF[~df_LOPF.type.str.contains('Anaerobic Digestion')]\n",
    "    df_UC = df_UC[~df_UC.type.str.contains('Anaerobic Digestion')]\n",
    "\n",
    "    # save the dataframes to csv\n",
    "    df_UC.to_csv('UC_data/generators.csv', index=True, header=True)\n",
    "    df_LOPF.to_csv('LOPF_data/generators.csv', index=True, header=True)\n",
    "\n",
    "    renewables.write_marine_generators(year, scenario, FES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmet_load():\n",
    "    # ADD NEW GENERATOR FOR UNMET LOAD\n",
    "\n",
    "    # get generators\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    df_LOPF = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    # # check names are unique for LOPF\n",
    "    # duplicateDFRow = df_LOPF[df_LOPF.duplicated(['Unnamed: 0'], keep='first')]\n",
    "    # for i in range(len(duplicateDFRow.index.values)):\n",
    "    #     print(df_LOPF['Unnamed: 0'][duplicateDFRow.index.values[i]])\n",
    "    #     df_LOPF.at[duplicateDFRow.index.values[i], 'Unnamed: 0'] = (\n",
    "    #         df_LOPF['Unnamed: 0'][duplicateDFRow.index.values[i]] + '.1')\n",
    "    #     print(df_LOPF['Unnamed: 0'][duplicateDFRow.index.values[i]])\n",
    "\n",
    "    path_UC = 'UC_data/generators.csv'\n",
    "    df_UC = pd.read_csv(path_UC, index_col=0)\n",
    "\n",
    "    # add one to the UC problem\n",
    "    dic_unmet = {'carrier': 'Unmet Load',\n",
    "                 'type': 'Unmet Load', 'p_nom': 999999999,\n",
    "                 'bus': 'bus', 'marginal_cost': 999999999,\n",
    "                 'committable': True, 'min_up_time': 0,\n",
    "                 'min_down_time': 0, 'ramp_limit_up': 1,\n",
    "                 'ramp_limit_down': 1, 'p_min_pu': 0,\n",
    "                 'up_time_before': 0, 'start_up_cost': 0,\n",
    "                 'p_max_pu': 1}\n",
    "    df_unmet = pd.DataFrame(dic_unmet, index=['Unmet Load'])\n",
    "    df_UC = pd.concat([df_UC, df_unmet])\n",
    "\n",
    "    df_UC.index.name = 'name'\n",
    "\n",
    "    # for LOPF need to add to each bus\n",
    "\n",
    "    # read in all buses with loads\n",
    "    df_buses = pd.read_csv('LOPF_data/loads.csv', index_col=0)\n",
    "    # add to each bus\n",
    "    for bus in df_buses.bus.values:\n",
    "        dic_unmet = {'carrier': 'Unmet Load',\n",
    "                     'type': 'Unmet Load', 'p_nom': 999999999,\n",
    "                     'bus': bus, 'marginal_cost': 999999999,\n",
    "                     'ramp_limit_up': 1, 'ramp_limit_down': 1,\n",
    "                     'p_max_pu': 1}\n",
    "        index = 'Unmet Load ' + bus\n",
    "        df_unmet = pd.DataFrame(dic_unmet, index=[index])\n",
    "        df_LOPF = pd.concat([df_LOPF, df_unmet])\n",
    "\n",
    "\n",
    "    df_LOPF.index.name = 'name'\n",
    "\n",
    "    df_UC.to_csv('UC_data/generators.csv', header=True)\n",
    "    df_LOPF.to_csv('LOPF_data/generators.csv', header=True)\n",
    "\n",
    "\n",
    "    # DEFINING A NEW FUNCTION TO AMEND 'WIND OFFSHORE' TO 'FLOATING WIND' AT SITES I, E, F, G, NE8, NE7, E3, E2, NE1, E1, NE2, NE3, NE6, N2, N3 - FOR LOPF ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace61e35",
   "metadata": {},
   "source": [
    "def floating_wind():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e823d5",
   "metadata": {},
   "source": [
    "    # read list of generators\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    df_FW = pd.read_csv(path, 'type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92891280",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "    # for sites I, E, F, G, NE8, NE7, E3, E2, NE1, E1, NE2, NE3, NE6, N2, N3, change the type to 'Floating Wind'\n",
    "    df_FW = d_FW.replace('I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f39d4",
   "metadata": {},
   "source": [
    "    # read in all buses with loads\n",
    "    df_buses = pd.read_csv('LOPF_data/loads.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cc410",
   "metadata": {},
   "source": [
    "    # add new type of 'Floating Wind' to each bus, under the carrier of 'Offshore Wind'\n",
    "    for bus in df_buses.bus.values:\n",
    "        dic_floating_wind = {'carrier': 'Wind Offshore',\n",
    "                     'type': 'Floating Wind', 'p_nom': 999999999,\n",
    "                     'bus': bus, 'marginal_cost': 999999999,\n",
    "                     'ramp_limit_up': 1, 'ramp_limit_down': 1,\n",
    "                     'p_max_pu': 1}\n",
    "        index = 'Floating Wind' + bus\n",
    "        df_floating_wind = pd.DataFrame(dic_floating_wind, index=[index])\n",
    "        df_LOPF = df_LOPF.append(df_unmet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a707a29",
   "metadata": {},
   "source": [
    "    df_LOPF.index.name = 'name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a2ffe",
   "metadata": {},
   "source": [
    "    df_LOPF.to_csv('LOPF_data/generators.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a99e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_generation_buses(year):\n",
    "\n",
    "    # get generators\n",
    "    path = 'LOPF_data/generators.csv'\n",
    "    df_gen = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    path = 'LOPF_data/generators-p_max_pu.csv'\n",
    "    df_gen_p = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    carriers = ['Wind Offshore', 'Wind Onshore', 'Solar Photovoltaics', 'Large Hydro', 'Small Hydro', 'Interconnector', 'Tidal lagoon', 'Tidal stream', 'Wave power']\n",
    "    buses = df_gen['bus'].unique()\n",
    "    df_list = []\n",
    "    df_gen_p_list = []\n",
    "\n",
    "    for c in carriers:\n",
    "        for b in buses:\n",
    "            df_carrier_bus = df_gen.loc[(df_gen.carrier == c) & (df_gen.bus == b)]\n",
    "            carrier_bus_aggregated = df_carrier_bus.p_nom.sum()\n",
    "            index = [c + ' ' + b]\n",
    "\n",
    "            # change the p-max_pu\n",
    "            list_of_sites = df_gen.loc[(df_gen.carrier == c) & (df_gen.bus == b)].index\n",
    "            gen_p_bus = []\n",
    "            for gen in list_of_sites:\n",
    "\n",
    "                try:\n",
    "                    gen_p_bus.append(df_gen_p.loc[:, gen] * df_carrier_bus.loc[gen, 'p_nom'])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            try:\n",
    "                df_gen_p_new = pd.concat(gen_p_bus, axis=1)\n",
    "                df_gen_p_new['sum'] = df_gen_p_new.sum(axis=1)\n",
    "                df_gen_p_new[c + ' ' + b] = df_gen_p_new['sum'] / carrier_bus_aggregated\n",
    "                df_gen_p_list.append(df_gen_p_new[c + ' ' + b])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            # change generators\n",
    "            if carrier_bus_aggregated > 0:\n",
    "                df_carrier_bus = pd.DataFrame([df_carrier_bus.iloc[-1, :]], index=index)\n",
    "                df_carrier_bus.p_nom = carrier_bus_aggregated\n",
    "                df_list.append(df_carrier_bus)\n",
    "\n",
    "        df_gen = df_gen[~df_gen.carrier.str.contains(c)]\n",
    "\n",
    "    df_gen_p = pd.concat(df_gen_p_list, axis=1)\n",
    "    df_gen_p = df_gen_p.fillna(0)\n",
    "    # just to ensure no negative values\n",
    "    df_gen_p[df_gen_p < 0] = 0\n",
    "    # add in interconnectors p_max_pu\n",
    "\n",
    "    df_gen_res = pd.concat(df_list)\n",
    "    # add in new generators\n",
    "    df_gen = pd.concat([df_gen, df_gen_res])\n",
    "       \n",
    "    df_gen.to_csv('LOPF_data/generators.csv', header=True)\n",
    "    df_gen_p.to_csv('LOPF_data/generators-p_max_pu.csv', header=True)\n",
    "    if year < 2021:\n",
    "        # fix interconnectors\n",
    "        # inter_cols = [col for col in df_gen_p.columns if 'Interconnector' in col]\n",
    "        # print(inter_cols)\n",
    "        # df_interconnectors = df_gen_p[[inter_cols]]\n",
    "        df_interconnectors = df_gen_p.filter(regex='Interconnector')\n",
    "        df_interconnectors.to_csv('LOPF_data/generators-p_min_pu.csv', header=True)\n",
    "\n",
    "    if year >= 2021:\n",
    "        # check if generators-p_min_pu exists and delete if so\n",
    "        # used in historical simulations but not wanted in future sims\n",
    "        try:\n",
    "            file = 'LOPF_data/generators-p_min_pu.csv'\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            file = 'UC_data/generators-p_min_pu.csv'\n",
    "            os.remove(file)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    year = 2025\n",
    "    tech = 'CCS Gas'\n",
    "    scenario='Leading The Way'\n",
    "    FES = 2022\n",
    "    future_capacities_dict = future_capacity(year, tech, scenario, FES)\n",
    "\n",
    "    # future_coal_p_nom(year)\n",
    "    # tech = 'Gas'\n",
    "    # future_capacity(year, tech, FES)\n",
    "    # tech = 'CCGT'\n",
    "    # tech = 'OCGT'\n",
    "    # future_gas_p_nom(year, tech, FES)\n",
    "    # future_nuclear_p_nom(year, FES)\n",
    "    # future_oil_p_nom(year, FES)\n",
    "    # future_waste_p_nom(year, FES)\n",
    "    # future_gas_CCS(year, FES)\n",
    "    # future_biomass_CCS(year, FES)\n",
    "    # future_hydrogen(year, FES)\n",
    "    # merge_generation_buses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261410f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
